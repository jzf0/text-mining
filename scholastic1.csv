Transcripts
"For an overactive student, daydreaming through the history of Ceramics, sleeping through an Art Appreciation lecture, or discussing the meaning of Aesthetic, is not one’s idea of fun or learning. At the end of the semester, the student hasn’t learned a thing. This boring arts class is basically a filler, simply intended to fill the one and only credit required for graduation, and out of it came one, lonely ceramic bowl. 
Unfortunately, this is the story of millions of American students — the story of art education that is barely arts, and hardly education. American art education is failing. The definition of art education prevalent in the United States is an idea dissimilar from the rest of the world. It’s an idea that builds upon the ideas of literature, science and mathematics, and not the creativity and beauty that arise within a treble clef and a brushstroke. It’s time to bring back art education effectively, with a full and mandatory curriculum, classes that are independant from core categories, required in public education, and available for all. It’s time to take the education out of the arts, and put the arts back into the education. 
The definition of arts education has devolved into ambiguity over the last 40 years, but the National Center of Education Statistics has grouped art education into four main categories, classes that involve visual arts, music, drama, and dance (Parsad et. al, 2012). In general, art education, which includes at least one of these four categories, is offered by 91% of American public schools (Parsad et. al, 2012) — but what are the more detailed constraints? 
Upon closer review, the the idea of art education is a lot less widespread today. In a 2009-2010 school year study by the National Center for Education Statistics, within the 91% of public schools that offer art education, 93% offered visual arts, 91% offered music, 14% offered dance, and 48% offered drama and theatre (Parsad, 2012, p.5). Although the music and visual arts percentages are high, the number of schools who offer dance and theater is dismal. That means that less than 50% of American public schools offer half of art education. It only gets worse form there. Only 57% of all secondary schools reported that a course in the arts was a necessary requirement for graduation, and the schools that did require art courses for graduation, about 70% reported that one, singular credit was required (Parsad, 2012, p.5). That’s about a semester of art classes. Already, that initial 91% has been cut in two, cut in two again, and almost cut in two after that. 
What ever happened to American art education, and how did it become like this — a skeleton of a true and comprehensive curriculum? To answer this question, it’s necessary to trace art education all the way back to the 1830s, when the idea was first pioneered. It started with Horace Mann, who not only trail-blazed the idea of art education, but also the idea of public schooling entirely. Mann defined society as a collection of strong-minded, self-governing individuals. According to Mann, one of the best ways to elevate a strong-minded society is by promoting mental growth, and thus emerged a teaching pedagogy which involved the furthering of mental strength; Mann claimed that powerful mental structures trump physical and emotional structures. One of the key points that Mann emphasized in education was that the arts could build upon all structures of self — for example, singing and visual art involved physical improvement through lung power, emotional improvement through “peace, hope and affection”(Raber, 2017), and mental improvement by noticing mathematical relationships between notes and tones. Art education hits all of Mann’s major points: physical, emotional, and mental. 
So, art education was implemented along with the nation’s first public schooling system. For 70 years, the program survived successfully. (Raber, 2017). However, in 1893, the arts were omitted from the public school system as they lacked “mental disciplinary value”(Raber, 2017). They weren’t regulated enough, and didn’t have enough worth in life to actually be taught, the 1893 Committee of Ten argued. 
The Civil War brought with it a new era in intellectualism and teaching philosophy, and art education was once more reinvigorated by Hegelian philosophers, who argued that the principles of repetition within the arts are simulating the “the stages of the individual’s induction into civilization”(Raber, 2017). Continued momentum built during the Progressive Era, as philosophers like William Toney Harris claimed that a full education means that all lessons, including sciences, mathematics, literature, and the arts, build upon one’s previous knowledge and interest, forming seamlessly into one curriculum that is based upon expressiveness and individuality. 
    The Cold War is when art education began its gradual and eventual decline. In 1957, the launch of Sputnik 1 seemed to leave the United States behind in an endless competition for worldwide superiority (Raber, 2017). Education was blamed; politicians looked for a way to completely refurbish the United States educational system, and it began with completely forgetting about art education. Suddenly, everything became occupation-based and individualistic. While focusing on the topics of science and math, the idea of a “talented elite” was introduced; now, the majority of the people would receive a rudimentary academic training in core classes, whereas the gifted best would receive special and higher quality occupational education, based on test score results. (Raber, 2017).
This system was met with vengeance by American educators, who argued that the arts were indeed an occupation, artists were talented specialists who existed to depict human experience and life itself, and thus every student needed this education (Raber, 2017). Their efforts failed; many aspects from the Cold War antiquated system carried over to America’s educational system decades afterwards, including the idea that art education can only be reserved for talented students. 
And such came the art education that America sees today. In 2008, art education programs were slashed across the country and thousands of arts specialists were laid off as the country began to collapse economically; in 2010, the Common Core State Standards Initiative was introduced. Astonishingly, visual arts were entirely omitted from the program (Raber, 2017). This widespread restructuring had widespread effects on students with low socioeconomic class; In 1999, 100% of students that qualified for free or reduced lunches (such that their household income was 100%-185% poverty level) were offered music education. In 2012, that figure was a sad 81%. Additionally, from 2008 to 2012, the percentage of schools offering drama dropped from a melancholy 20% to a mouth-dropping 4% (Association of American Educators, 2012). 
This problem has evolved into an equity issue; the depth of course offering, the availability of a full education in the arts has widely been available only to those with a high socioeconomic class, leaving students who live on the brink of the poverty line with limited or no access to art classes at their school (Association of American Educators, 2012). Minorities were among the most effected by the cuts — today, only 26.2% of African American students have access to art classes (Gregory, 2016).
Many studies about arts education aimed their focus directly at underserved schools. The National Endowment for The Arts is an independant, federally funded agency that helps expose students-at-risk to high-quality arts programs (Miller, 2017). In 2008 and 2016, a national assessment known as the Nation’s Report Card was conducted, and students’ knowledge of the arts (visual arts and music) were assessed. 
The results were quite interesting; in a test out of 300 points, the average score for Hispanic students increased by 6 points, from 129 to 135 points since 2008. Additionally, students that qualified for a free or reduced lunch program, such that they had a household income of 100 -185% poverty level, had a 5-point increase in their scores in the 8 year difference (Association of American Educators, 2012) Organizations like the National Endowment for the Arts, the Americans for the Arts, and Arts for Life are responsible for this success. Not only did these organizations reach out to underserved youth, but they were able to demonstrate that students who are exposed to art education experience positive growth at school. 
To add more fuel to the fire, these organizations are being bulldozed as President Trump reduces their budget — over 30 million dollars of funding has been cut from The National Endowment for the Arts; its budget was diminished from $176 million to $146 million (Joseph, 2016).
    As more and more art education programs are tossed into the trash, more and more research has proven the positive effects of an arts education on children. For example, there is a high correlation between students that are exposed to a high level of art education and cognitive ability. In 2002, neuroscientists found that students that had been exposed to arts programs had advanced attention skills and developed creative thinking solutions; students were able to identify complex relations between musical or artistic qualities. This attention towards this difference in art translated into innovation(Bush, 2018). Put in simpler terms, students became more creative and critical thinkers because of their development in the arts. 
This is only confirmed by the demands of the 21st century workforce. A 2006 report, titled Are You Ready to Work?, found that skills like teamwork, critical thinking, and problem solving are all skills that surpass other skills like mathematics, science, and reading comprehension. Not only that, but creativity and innovation was listed among the top five attributes (Bush, 2018). As mentioned, having exposure to arts education dramatically improves critical thinking skills, problem solving, and creativity. So, top attributes of workers in today’s society are bolstered by art education, the needs for it in the education system in America is only amplified. 
One might claim that classes in the arts distract from other, more important subjects; Lavine (2018) argued that “a student is required to take a certain number of art classes, which can be taking valuable time away from other, more enriching courses [such as] ...AP or Honors courses.” This, however, is directly opposing all of research done by influential organizations such as the National Endowment for the Arts. When students were exposed to arts education, their scores on tests rose significantly, as evident from the score improvement in 2008-2016. In addition, AP classes aren’t for everyone — they require a high level of commitment and academic challenge that is not available to students-at-risk, and they are weighed differently by different colleges (Cooper, 2017). However, art education has been proven to be beneficial to all students.
Arts education is especially powerful, and even necessary, when it comes to kids with Autism. Many students were implemented into a dance program, and enormous changes were observed. Not only did the autistic students increase their physical ability, coordination, and strength, but the program was also influential by “enhancing overall wellbeing by reducing potential risks which are often associated to loneliness, isolation and poor social identity” (Roncaglia, 2018, p.2). Along with athletic improvement, students also noted powerful mental growth as well. Roncaglia also noted that the arts education programs help children overcome barriers that are seen as roadblocks to their development, like social and emotional skills (2018, p.6). By undergoing live performances, the children were able to simulate real-life occurrences through their performances, meaning that they could have a space in which they could grow and develop without pressure.
So the arts are essential to growth and development in the life of any student. They improve cognitive ability, help improve scores on standardized tests, and especially help students at risk. But there is one puzzle piece missing: what is the most effective way that art education is applied? In what ways has it been proven to be successfully integrated into an entire public school curriculum? How can we do art education right? To further investigate these questions, it’s necessary to take a look at entire countries that have applied successful education techniques.
Take Finland, for example. Finland is a country that wasn’t doing so well in their schooling system. About 40 years ago, their education mingled in with its mediocre brother, the United States of America. Now, however, they differentiate from America in several ways; they only have 1 standardized test mandated, and have hour-long lunch hours, and can learn many different languages, but the real difference between the two education systems is, without a doubt, their arts education curriculum. 
Art education in Finland is entirely mandatory, and fully includes all four categories: physical education, music, visual arts, and crafts. Students have a wide variety of options including full drama opportunities, concerts, exhibitions, and cultural education. “Art is essential for personal development and wellbeing”(Busnatch, n.d, p.1), according to a report on Finnish art education. “Art plays an important role in promoting a child’s creativity, which applies to all subject areas...progressing from one level to another, it is goal oriented” (Busnatch, n.d, p.1). Consider this situation in a typical Finnish elementary classroom: 
“A class of first graders scampered among nearby pine and birch trees, each holding a stack of the teacher’s homemade laminated ‘outdoor math’ cards. ‘Find a stick as big as your foot,’ one read. ‘Gather 50 rocks and acorns and lay them out in groups of ten,’ read another. Working in teams, the 7- and 8-year-olds raced to see how quickly they could carry out their tasks” (Hancock, 2011). 
What here is different from learning multiplication tables in a classroom? What here is different from filling out a worksheet?
The answer is art. The students are required to be artistic, and are forced to use artistic techniques to learn. Students learn math in creative and fun ways (such as this situation, where students are required to collect outdoor materials), think creatively by singing songs, and exercise imaginative individuality by wearing costumes to school (Hancock, 2011). Success emerges from empowerment and individuality. It’s not based off of occupation, the standardized tests, or memorization. The Finnish education system has nailed what art education is supposed to be: a comprehensive, full curriculum. 
America’s art system is failing, but it’s not dead yet. A full art education in America is possible; it happened over 175 years ago when intellectuals and philosophers like Horace Mann explored the idea of public education itself. Over time, the idea of the arts have been slowly degraded and worn down by fear, superiority, and ignorance. But this fear and superiority can be overcome with research and implementation. Horace Mann was right. Art education isn’t just a wasteful class; it improves test scores, is valued in the workforce, and provides beneficial and growing environments for students at risk, and even improves attention and analyzation skills. 
Small steps can create big leaps; it’s time to provide more funding to art programs like the National Endowment for the Arts, add back art classes to underserved schools, and reintroduce drama and dance. Above all, art classes must be completely independant from core subjects and available to all students.It’s time to quash the idea that the arts are imprudent and wasteful. It’s time to equalize, time to integrate, time to innovate. Let’s open the floodgates to a creative, powerful, and artistic future. "
"“Joy is not in things; it is in us” – Richard Wagner. The exploration of happiness and joy initially appears as a simple endeavour; however, their connection and purpose exist separately. Google Dictionary defines happiness as ""feelings ranging from contentment and satisfaction to bliss and intense pleasure,"" whereas joy ""is a stronger, less common feeling than happiness"".  The constant search for this elusive “feeling” ultimately leaves people feeling empty. Yet, instead, those who experience a form of joy only found in times of hardship, selflessness, or personal sacrifice are not left empty, but rather experience something truly lasting. By specifically analyzing culture and socio-economic aspects of life, there exists a growing chasm between a life filled with moments of happiness and a life dependent on joy.    
 
Grasping happiness:
 
Is it something innately within us, like an instinct, that drives us toward seeking a moment of happiness at all cost? Many Americans believe happiness is a basic human right, something wildly attainable, something readily available. We believe that with hard work or the right connections or the correct address or education, more happiness awaits just ahead. Yet along with this set of beliefs comes several universal questions: What is happiness? How can I assure my life-long happiness? And, is happiness really for everyone no matter their passport?  Ruut Veenhoven from Erasmus University Rotterdam, believes “happiness is a psychological surface phenomenon, which is at best a far cry from the real good” (Veenhoven 2). His research discusses happiness as a temporary high: once we receive something we truly desire, like a brand new car or house, that satisfaction only lasts momentarily, and once that desire is fulfilled, we simply desire another something of equal or greater value.  One way of feeding unhappiness may be to simply fill-up with something desired—thus, the instant popularity of Amazon delivery—anything delivered in 24-48 hours with a Prime membership. However, Veenhoven writes in his book, The Conditions of Happiness, that striving to be happy is “a far cry from real well-being” and that is where the chasm is exposed as we then try to define the idea of  “well-being” existing separately from simply being happy (Veenhoven 3). Humanity desires happiness, yet happiness is only temporary, and with such constant want of something new, we feel the rise and fall of emotions—desire, loss of desire, emptiness—and thus, left to wonder if we are fully living.
 
Pure joy, next level:
 
            One of the hardest concepts to grasp is the emotion of pure joy. The emotion seems entangled or synonymous with happiness and yet, pure joy may not actually involve being happy at all. For example, take the old grandmother, normally lonely doing the same thing day-in-and day-out suffering from arthritis, collecting her social security check for food. Yet she still says she doesn’t want any Christmas gifts because seeing her grandchildren open their gifts is all the joy she needs for the entire year. Is that moment watching her grandkids being happy a temporary emotion or is it a deep satisfaction that all is well in her family’s life, which in turn eases worry, reminds her of her priorities? Douglas Abrams, the co-author of Have You Renounced Pleasure?, defines the difference between being happy and having joy by stressing that one is connected to the other: “When we speak of experiencing happiness, we need to know there are actually two kinds. The first is the enjoyment of pleasure through our senses” and the other is experiencing “...happiness at a deeper level through our mind, such as love, compassion, generosity” (Abrams 53). To have pure joy means we require a; “deeper level of thinking” this “deeper level” requires a divergence from one’s comfort zone or a challenge that may cause pain. Society must understand that “while joy of the senses is brief, the joy at this deeper level is much longer lasting.  It is true joy” (Abrams 53). The old grandmother witnessing a deeper joy with her family is not focused on her temporary senses of pleasure, but rather on the memory of her husband, or a life fully lived, or the past dreams she once dreamed and are now fulfilled.  When someone returns to a memory of a time they felt love, it may have started out as happiness, but once the emotion is challenged, learns to endure in tough times, the memory of love turns from temporary happiness to a deepened joy, an enduring and permanent understanding.  
 
Culture and satisfaction:
 
Why is it that people in first-world-countries believe they are happier than those who live in the developing world?  Maybe it is innately human to view money or race or skin color as a sign of happiness. If money buys food, and food helps someone live another day, then money buys happiness because living with food should make someone more happy than dying of starvation. Or at least that train-of-thought sounds logical and would imply that all people in America are happier than all people in Uganda—untrue. Is it possible that what makes people happy in one country causes unhappiness in another? For instance, in Western societies, one’s income is considered important to one’s happiness as people truly believe it’s realistic to live in perpetual throws of endless happiness ingrained in us at such a young age—Santa, Easter Bunny, and Tooth Fairy are all designed to insure happiness to a child for the day or the hour.  Disney characters always live their ‘happily-ever-after’ and these fairy-tale endings inevitably brainwash the first world into believing there’s a fairy-tale ending for everyone. Yet, when there is not a fairy-tale ending, what’s the alternative? Unhappiness? Abram’s comments on the state of young people in the first-world by saying “so that’s why their satisfaction is very limited and brief” (Abrams 53). The root cause and conflict of searching for happiness around every corner or searching for a satisfaction that sustains our lives is a fruitless effort. 
As a former Ugandan street kid myself, I didn’t grow-up watching attractive, yet unrealistic stories until I was at least eight years old. Before someone brought a Disney movie to Kampala, Uganda, I looked forward to playing with my friends, not hoping or expecting the next best thing to fulfill me. I wondered instead if I was going to eat that day and when I did eat, how much more fun the dancing was that night. But when relief did arrive, there was a joy, a deep satisfaction that life is good and as long as we don’t have Malaria, we enjoy. Adam Alter, writer of Do the Poor Have Meaningful Lives? says, “people from poor countries tend to view their lives as more meaningful” because they are not brainwashed by what they watch on television. I agree whole-heartedly. A life lived seeing people have more stuff, more moments of laughing, more comfort, leads to more desire to fill sadness or discomfort with temporary things. Yet, Americans brought us shoes and told us we were poor, ironically ushering in temporary happiness, and simultaneously a reduction of joy. In reflection, I only remember the great moments like eating chicken on Christmas or having a warm soda for the first time, and of course, my best friends. Our lack of television led us toward a life of love and caring for one another, which in turn has led to my life having a deeper appreciation for simple things like a drinking fountain—water just flows out of the wall with literally no effort, and yet at my school, no one, absolutely no one, considers that happiness or joy—yet, it is both!
 
Socio-economic factors and our satisfaction:
            My American grandmother says, “money can’t buy you happiness, but it sure is nice to have when you need it.”  Greed then twists this concept into acquiring as much wealth ‘just in case we need it’? Pop culture acknowledges daily the difference between the crazed Kardashians and the rapper, Notorious B.I.G.’s song Mo Money Mo Problems; “I don't know what they want from me / It's like the more money we come across / The more problems we see.” In these simple lyrics even B.I.G. expresses his dissatisfaction with growing wealth and popularity—why is that?  Did he not initially believe his life would be better, happier, more fulfilled if he could simply rise-up as a pop culture icon?   
Does the fundamental belief in making more money not lead to the belief that we can have enough money to then take the time to find our true joy—painting or traveling or volunteering at a soup kitchen? The American Dream seems to allude to the fact that if we make enough money to not think about money, then can we live a life of deep satisfaction and joy. Bill Gates appears to be living a life of deep satisfaction as he continues to fund life-changing human dilemmas, but there’s no-telling that he does not simultaneously live riddled with guilt from knowing he has so much while others starve or die of curable diseases like Malaria—again, another chasm widens between a life of joy and a life of intense responsibility. In her Forbes article, “This Harvard Study Says the Happiest People Have More Time and Less Money”, Brianna Wiest discusses the optimal salary of an American.  She claims that when one makes too much (ie. Bill Gates), it leads to stress, a version of unhappiness; and if one makes too little, it also leads to stress and unhappiness. She explores the perfect scenario by analyzing the Gallup Institute data. Wiest says there’s a clear pattern as “people who have enough time are happier, less depressed, experience more joy, exercise more, eat better, are more productive, and are less likely to get divorced.” Their discovery is directly linked to economic incomes with “magic number being $75,000” and  some even believe the number is a bit lower, maybe closer to $50,000 since “when you jump from say $10,000 to $50,000, you notice a marked difference in your quality of life” and that “quality of life: changes to things that might bring happiness for a certain time”(Wiest). But somehow, the salary of $75,000 is the magic number for quality of life not being too much to dream outside of what one requires: pay the bills, yet not live in the stress of lower incomes which leads to lack of healthcare, etc. If this magic number is true, people need to be aware before they too suffer the great downfall of mankind—the drive to make money at all costs. 
 
Conclusion:
Humanity wakes-up everyday with a road map of choices: live by joy and seek enduring satisfaction, or live a life of happiness by filling days with temporary pleasures like a pretty outfit or popularity. By these simplified definitions, we thoroughly see the chasm between the two, but can we easily choose one or the other? Accordingly, we know that joy needs happiness to exist. However, does happiness require joy to exist? For example, to have a memory worth holding onto forever, turning into an enduring joy, one must have a happy moment along the way no matter how fleeting; thus, proving that a simple daily road map of choice is more complex. And, if everyone's efforts are focused on trying to be happy, they inevitably miss the point to life. Instead of chasing unrealistic levels of happiness, and no matter our passport or race or skin color, we should be in search of deep satisfactions flowing out of joy, fully aware that the equal and opposite emotion of happiness is self-inflicted unhappiness. Therefore, enjoy the simple moments of happiness as they come while living fully-aware that too much of them may destroy their ability to one day be enduring joy. "
"Riots have marked America since the country’s very beginning. Perhaps one most notably sewn into the fabric of American history is the Boston Massacre, in which British troops fired on a crowd of American protestors in a city fraught with tensions over British tax laws. Other notable riots in American history include the New York Draft Riots and Haymarket Square Riot, both in response to work-related grievances in the mid to late 19th century; the Zoot Suit Riots of 1943 during which white military personnel targeted black, Filipino, and Mexican men in Los Angeles; and the famous Stonewall Riots in 1969, when police and club-goers clashed outside of a gay bar, paving the way for LGBTQ+ rights. Race-related riots have also been frequent, many occurring as a reaction to police violence. These include the Stono Rebellion, the riots of the Red Summer of 1919, the Detroit Riots in 1967, and the Los Angeles Riots in response to the acquittal of the officers who beat Rodney King.

These riots did not appear out of thin air. Widespread anger is rarely baseless—communities full of comfortably employed people with full bellies do not see the collective rage and violence that marks periods of civil unrest. But are riots effective in combating the issues from which they stem? The issue is hotly contested—there is evidence to suggest that they often economically harm the communities in which they occur, while there are also historical examples of their success, including the riots after MLK’s death that led to the passing of groundbreaking civil rights legislation in the form of the Fair Housing Act of 1968.

This past summer has seen many riots catalyzed by the killing of George Floyd, reigniting the debate over the effectiveness of violence and looting in bringing about change. Those opposed to the Black Lives Matter movement often cite property damage and Martin Luther King Jr.’s tactics to prove that peaceful protest is the only “right” way to fight oppression, while supporters of BLM argue that the riots have led to victories within the movement, including the reopening of Elijah McClain’s case and banning of no-knock warrants in various states following the death of Breonna Taylor.

But while both of these views see riots as either effective or ineffective solutions to a problem, riots shouldn’t be seen as a potential solution at all—rather, they are the inevitable, very human reaction to state-condoned violence and oppression and should be treated as an indication to tackle underlying problems instead of as a means to do so. 

Riots are not, as many believe, mindless, unfocused mob rule—they are the mobilization of many individuals who share a common enemy or cause and who take to the streets for their movement specifically. When a riot takes place, group polarization and anonymity come into play—that is, people lose their sense of self when in a crowd of like-minded people, leading them to make more extreme decisions. All that is necessary to start a riot is underlying rage and frustration and a catalyst, like a blaze beginning with just a pile of tinder and a spark. In the case of the recent surge in the BLM movement, the tinder is the anger within the black community built up over centuries of systemic racism, like a powder keg pressurized to the point of explosion.

Humiliation is a recurring theme within this anger. There is humiliation in being slammed over the hood of a car by a police officer as your white counterparts stand by, untouched. There is humiliation in being denied a job just for having an African American-sounding name, in growing up and always seeing people with your skin color relegated to the roles of villains and sidekicks. There is humiliation in being the only person with your skin color in a boardroom, in listening to a backhanded compliment from a coworker that you “speak so well for someone like you,” in feeling the burning stares of white classmates who have never seen hair like yours. There is humiliation in repeated embarrassment and rejection within white society of your personhood, and there is humiliation every time you have to convince someone that your pain exists.

Those who condemn the riots yet claim to support the BLM movement in all other aspects miss the frustration at the very heart of the black community. The riots are a symptom of widespread oppression, and denying their validity denies the right of black Americans to be angry at their own oppression and mistreatment and to express themselves in accordance with that rage. 
Attempting to label riots as effective or ineffective treats them as a political tool, but that perspective completely misses what riots are: a natural response to generations of pent up anger and dehumanization. When someone cries out after their arm is broken, they do not believe their own cry will get them to the hospital or heal their injury—they are simply reacting, responding naturally as a human who feels pain. Pain within the black community stretches back centuries, and it runs far deeper than a broken arm, weaving itself through the nerves and sinews of society like a disease that has been allowed to ravage this country for far too long. 

Many condemn one type of violence while ignoring the other—that is, systemic, state-sanctioned violence whose victims respond with their own fervent defense. Slavery is violence. The war on drugs is violence. Police brutality is violence. Redlining, segregation, mass incarceration, racial achievement gaps, and microaggressions are violence. Yes, riots are violence too, but they occur at the other end of the balance of power, an outcry among African Americans over scales that are so overwhelmingly stacked against them. Author Ta-Nehisi Coates wrote eloquently in 2015 in response to the Baltimore Riots about the government’s response to civil unrest: 
When nonviolence is preached as an attempt to evade the repercussions of political brutality, it betrays itself. When nonviolence begins halfway through the war with the aggressor calling time out, it exposes itself as a ruse. When nonviolence is preached by the representatives of the state, while the state doles out heaps of violence to its citizens, it reveals itself to be a con. And none of this can mean that rioting or violence is ""correct"" or ""wise,"" any more than a forest fire can be ""correct"" or ""wise."" Wisdom isn't the point tonight. Disrespect is. In this case, disrespect for the hollow law and failed order that so regularly disrespects the community.

There is an attempt by many, often those who have the most to lose should widespread civil unrest overturn the existing social hierarchy, to channel the anger evident in riots into more manageable outlets. Peaceful protest is lauded as the only way to achieve equality—even though peaceful protest is often met with similar discontent—because peaceful protest does much less to destabilize the status quo than does rioting. The founding fathers feared “mobocracy” as much as current politicians and CEOs fear the looting of a Target, and those in power have influenced large parts of the public into fearing their own collective power.  
Riots shouldn’t be measured by their “productivity,” by the laws passed or cases resolved in their wake. Riots shouldn’t be seen as a tool for fighting oppression, or a replacement for the widespread reorganization of society that is needed to combat systemic racism. Riots should be observed and reported on, rioters acknowledged as human beings exercising their right to feel angry over mistreatment, and underlying causes of riots seeked out and addressed.

The Kerner Commission, a report identifying poverty, unemployment, voter suppression, violent policing, and other forms of institutionalized racism as major causes behind the unrest in cities across America during the summer of 1967, states, “What the rioters appeared to be seeking was fuller participation in the social order and the material benefits enjoyed by the majority of American citizens. Rather than rejecting the American system, they were anxious to obtain a place for themselves in it.” All non-black Americans should be examining and working to dismantle the institutions that keep our black brothers and sisters from enjoying the opportunities we are afforded, not berating them for demanding their rightful seat at the table. 

Malcolm X once said, “You can’t separate peace from freedom because no one can be at peace unless he has his freedom.” This has been echoed in recent chants from the BLM movement of “No justice, no peace.” As long as oppression continues, riots are inevitable. Perhaps what is to be most feared is not the riots themselves, but a world in which black Americans do not riot in response to the killing of their family members on the streets, the locking up and exploitation of their brothers and fathers, and the continued weight of chains that have yet to be lifted. "
"Dear Governor DeWine,
     I am writing to you about the unveiling of your “Strong Ohio” bill this past October. In your announcement of the bill, you explain that the inclusion of red-flag laws and universal background checks in your proposal would be ineffective given the stance of most republican congressmen. I agree with your reasoning that this bill is much more likely to pass in the Ohio General Assembly than a bill that includes a “red-flag” gun seizure law; this strategic move is likely to appeal to many republicans and does not largely take away from the effectiveness of the bill. Additionally, I agree with your assessment that the bill still has the potential to save lives without the integration of these clauses. Provisions such as increased penalties and punishments for crimes such as selling guns to minors, selling or providing guns to someone legally prohibited from possessing a gun, and illegally having a firearm in one's possession are a step in the right direction. Furthermore, the addition of “pink slip” laws that provide court-ordered treatment to those suffering from mental health disorders and drug and alcohol addictions before a mandatory probate court hearing allows the court to remove guns from the possession of those who are a danger to themselves or others. However, I believe that your decision to omit universal background checks in an attempt to make the bill more legislature-friendly has significantly undermined many of the bill’s preventative measures. While the “Strong Ohio” Bill contains gun control advancements that would be an improvement from what currently exists in Ohio, the absence of universal background checks renders many of the bill’s proposals ineffective. I would like you to consider the possibility of implementing these preventative measures in your gun control policy.
     Although the odds of a person dying due to a mass shooting in the U.S. are only one in 11,125 (which is still the 32nd most common cause of death), the probability that a person will die of assault by gun is one in 315, making gun violence a leading cause of death in America (Gould and Mosher). Though I have, fortunately, never been directly impacted by gun violence, my lack of experience does not influence my ability to understand the seriousness of the situation. Out of the 65 shooting-related incidents at schools this year, 36 of them occurred at high schools (“Incidents by School”). As a high school student, the issue of gun violence and how it can be prevented is very important to me. Tragedies like the Stoneman Douglas High School shooting in Parkland, Florida have shown that the U.S.’s laws can not protect school-aged children and teenagers. As we are affected by the lack of gun control and effective safety measures in America, we have no choice but to participate in gun reform discussions and debates to improve our own safety. In addition to school shootings, firearm-related suicide is another issue affecting teens. Over 1,000 children and teens’ lives are lost each year due to firearm suicide, a rate that has increased 82% over the past ten years (“Firearm Suicide”). In addition to problematically high suicide rates, gun-related homicide is another leading cause of death in teenagers: “Annually, nearly 2,900 children and teens (ages 0 to 19) are shot and killed, and nearly 15,600 are shot and injured—that’s an average of 51 American young people every day” (“Impact of Gun Violence”). However, the impact of gun violence on America’s younger generation goes beyond direct wounds. Everytown for Gun Safety Support Fund estimates that “about 3 million children witness a shooting each year” (“Impact”). Oftentimes incidents like these occur without warning when they are least expected. 
     On August 4th in Dayton, Ohio, a gunman killed nine people, including his own sister, and injured 24 others in the 30 seconds it took for police to take him down (Martin). The massacre occurred in an open area called the Oregon District where many people go to enjoy dinner and live music. One of the victims, 25-year-old Nick Cumer, had been out celebrating his new job offer at the Maple Tree Cancer Alliance before he and his friends were brutally murdered (Allyn). As you and the Mayor of Dayton, Nan Whaley, attended a vigil following the shooting, crowds of people demanded that you do something about the lack of legislation protecting Ohioans from gun violence. In an interview with Cleveland.com, you referred to your own bill as better than the alternative of “doing nothing” (Darcy). While this may be true, it is an unbelievably low standard to aim for. When dealing with the possibility of more innocent people like Nick dying prematurely due to gun violence, a bill that is “better than nothing” is not good enough. Additionally, since the bill does not create a requirement for background checks on private gun sales, there is a large loophole that criminals can still use to acquire weapons. In fact, “Around 80% of all firearms acquired for criminal purposes are obtained through transfers from unlicensed sellers, and 96% of inmates convicted of gun offenses who were already prohibited from possessing a firearm at the time of the offense obtained their firearm from an unlicensed seller” (“Universal”). Due to this discrepancy, it is unlikely that the bill’s implementation of an optional “seller protection certificate”, which can be requested from sheriffs’ offices to prove to sellers that one has passed a background check, will be effective. While the bill provides an incentive for sellers not to sell firearms to those who do not have seller protection certificates, there is no guarantee that transactions without them won’t happen. Instead of stopping the problem at the root with universal background checks, The “Strong Ohio” bill waits for the criminal to acquire the gun, possibly commit the crime, and then punishes the buyer and the seller with harsher sentences once they are caught.
Although the removal of “red-flag” laws from this bill was a logical decision, the additional elimination of a universal background check policy was unnecessary and ill-advised. 
     A recent Quinnipiac University poll shows that 90% of Ohioans support universal background checks for all gun buyers (“Ohio Voters”). This widespread support includes both gun owners and non-gun owners. Statistics have shown that, overall, even the most pro-gun citizens of the U.S. support background check laws: “Strong support for background check laws has also been measured among NRA members, with at least 69% supporting comprehensive background checks” (“Universal”). These perspectives were not adequately taken into consideration when determining whether his bill would be able to pass Ohio’s legislature. While it is clear that Republican House Speaker Larry Householder is going to be a challenge to persuade to support this bill, it appears that he and most other Republicans are mainly concerned about the possibility of “red-flag” laws being put into practice. Both lawmakers like Householder and officials such as Dean Rieck of the Buckeye Firearms Association were particularly concerned about the idea of gun seizure and are now pleased to see that the bill doesn’t have a “red-flag” law, if still harboring a few reservations about the proposals. According to Lieutenant Governor Jon Husted, a red-flag law would be “inadequate and unworkable” due to the extensive amount of time it would require, resulting in the endangerment of both the person in question and law enforcement (Kasler and Ingles). However, the same issue cannot be said for universal background checks; they have proved to be both efficient and effective: “In at least 90% of cases, firearm background checks processed through the National Instant Criminal Background Check System (NICS) are resolved immediately” (“Universal”). Most importantly, the positive impact of implementing universal background checks outweighs all shortcomings and difficulties that it would require to pass the law. It has been shown that states without background check laws are significantly more unsafe than those that utilize them: “States without universal background check laws export crime guns across state lines at a 30% higher rate than states that require background checks on all gun sales” (“Universal”). Eliminating the loophole in our gun control laws that most criminals exploit to obtain firearms is worth the extra time and effort it would take to convince the Ohio legislature of its importance. 
     While at first glance the “Strong Ohio” Bill seems like a solid, if slightly subdued, approach to improving gun control in Ohio, its power is significantly diminished by the lack of a universal background check law. Your choice to exclude this safety measure from your gun control policy fails to extinguish the method by which the majority of criminals acquire weapons. Although the bill would put certain punishments in place to encourage private sellers not to sell firearms to those without background checks, this approach to the problem is backwards and doesn’t target the issue directly enough to prevent it from occurring in the future. The welfare of Ohioans is worth more than a bill that just barely improves our gun control laws in an attempt to please calls for reform in the wake of the latest mass shooting or other firearm-related tragedy. Nationwide and in Ohio, gun owners and non-gun owners alike support the use of universal background checks to assure the protection of innocent people from gun violence. While it is clear that passing a less conservative bill in the Ohio legislature will be difficult, we can do so with enough support. It is essential that Ohioans who want a safer future for their state voice their support for the implementation of universal background checks into Ohio’s gun control policy. However, it is also the job of the state government to listen to what the people are saying and act accordingly. For the sake of all Ohioans, I urge you to include universal background checks in your gun control policy."
"Despite having just under 5% of the world’s population, the United States houses 22% of the world’s prisoners.[i] It is a statistic often repeated when discussing prison reform, and for good reason; the “land of the free,” for all of its boasts about possibility and equality of opportunity, has the highest incarceration rate in the world at 698 per 100,000, a burden that falls disproportionately on blacks and Hispanics.[ii] These statistics seem to indicate a country struggling to maintain order and contain a slew of criminals, but since 1991, the violent crime rate has fallen substantially, while the number of people incarcerated has increased by 50%.[iii] There is something fundamentally wrong with how we operate our prisons, something fundamentally wrong with a system that locks up millions of people each year yet has “liberty and justice for all” in its pledge. Do we truly believe that those millions deserve their place behind bars, that the label of criminal constitutes treatment oftentimes unbelievably cruel? The American prison industrial complex traps individuals in a cycle of incarceration that is rooted in poverty and perpetuated by trauma and disadvantages accrued behind bars—it is a system that emphasizes punishment over rehabilitation in a way that denies the potential of people to better themselves.

The path to prison lays its initial brickwork in the poorest neighborhoods of America, where a vicious cycle of poor education, unemployment, debt, and violence traps people in poverty and drives them towards crime. These communities, largely with high black populations due to the pervasive effects of segregation and economic disenfranchisement, lack the tools key in determining an individual’s success. In an environment so devoid of opportunities, children struggle from the onset—30% of children living in poverty do not earn a GED, which makes them 3.5 times more likely to be arrested than their counterparts who finish high school.[iv] A bad education then hinders people from finding jobs, and extremely poor neighborhoods see an unemployment rate among working-age men nearly 20 percentage points higher than the national average.[v] Higher rates of violent crime plague these areas, and they funnel straight into the prison system for many; the pre-incarceration earnings of incarcerated people are 41% lower than that of their similarly-aged, non-incarcerated counterparts.[vi] With no other options, some turn to crime simply to survive, lacking the luxury of choice with which those born into better circumstances are privileged. 

The strong links between poverty and crime are intuitive: those in desperate situations will turn to more drastic measures to support themselves, taking actions that would not be necessary for someone who can afford to feed their family, pay for healthcare, or resolve their debts. There is a certain stigma attached to poverty in a country that, according to many, affords equality of opportunity to all. When someone has failed, it is seen as a failure of the individual to make use of their resources and pull themselves up by their bootstraps, but with almost 40 million Americans living in poverty, it may be more apt to conclude that poverty, and by extension crime, are failures of society.[vii] The individual is burdened with racial, class, and gender wealth gaps established long before they are even born. But the wide disparities within this country have not been met with concerted efforts to lift people out of poverty and tackle crime at its root—instead, an industry has sprung up, one worth billions that throws people behind bars as a punishment for their desperation.

America’s prison system maintains an environment that can only be described as cruel and unusual, an environment largely focused on punishment and hostile to individuals hoping to turn their life around. Solitary confinement, claimed as torture by the UN, is a practice used routinely by prisons wherein a prisoner can be held alone in a cell for 22-24 hours per day, with devastating effects: heightened risk of panic, anxiety, rage, depression, hallucinations, and suicide.[viii] Additionally, psychological distress, cramped conditions, and idle time caused by short staffing, overcrowding, and the cutting of rehabilitation programs make violence commonplace.[ix] Mika’il DeVeaux describes a sickening instance of violence he witnessed while serving 32 years behind bars:
I still see the image of a person being hit at the base of his skull with a baseball bat on a warm, sunny afternoon during recreation hours. The entire scene plays like a silent movie. He is smashed in the back of his head, crumbles, and falls to the ground. While he lays helpless on the ground, his head is smashed again and again until the sight of blood seems to satisfy his attacker. I watch as the perpetrator then calmly returns the baseball bat to the location where he had retrieved it and just walks away as if nothing had happened, while others entering the yard area walk around the lifeless body.[x]
It would be naive to imagine someone witnessing such a horrifying situation, one that plagues their memory years later, will leave completely unscathed. This violence may be seen as a result of what some deem as the “natural tendencies” of criminals, but in actuality, it stems from the dehumanization of people and neglect of their needs. When people are treated like animals—when they are battered and made to feel as if they mean nothing to the outside world—they will naturally respond to their suffocating circumstances, whether it be through violence, complete withdrawal from their fellow prisoners, mental deterioration, or something far worse. 10-25% of US prisoners suffer from a serious mental illness in contrast to 4.6% of total US adults.[xi] Rates of PTSD, paranoia, panic attacks, and depression are higher among inmates, as are the rates of chronic physical ailments such as diabetes, hypertension, and asthma.[xii] America’s prisons make the already weak weaker, responding to pain not with treatment but with trauma. They do not “whip people into shape”—they damage people further, making those who are broken less likely to ever find success or happiness. This is a far cry from rehabilitation, something that has become more of a luxury than a right.

And punishment does not end upon release. Once someone leaves their distressing situation behind bars, they enter a world in which their label subjugates them to the role of a second-class citizen, burdened by institutional discrimination and stigma to the point of ostracization even by friends and family. Felons can lose their voting rights while incarcerated and/or during a period after in most states, and in eleven states they lose their voting rights indefinitely or must go through extra steps to regain them.[xiii] Additionally, many former prisoners fall right back into poverty and unemployment, with just 52.6% of prisoners employed six months after their release and almost half making less than $500 annually one year after they leave the system.[xiv] If these wealth disparities sound familiar, that is because they are almost identical to the situation many felons find themselves in before they are incarcerated—the position of desperation and frustration that pushed them into crime in the first place. But now they face the additional stigma of being an ex-con, which can affect employment (a criminal record reduces the likelihood of a callback or job offer by nearly 50%, with the negative effects more heavily falling on black people), access to food stamps (drug traffickers subject to a ban on food stamps in Florida were about 9% more likely return to prison than those who had access to SNAP), and family stability (children with an incarcerated father are more likely to be incarcerated themselves).[xv] These burdens, which haunt felons long after they have served their time, destroy communities and only worsen the difficult circumstances that breed crime. People inevitably end up back in the system, and their children end up in the system, and so do their children’s children, until entire families have been sapped of their potential and robbed of what little wealth they had to begin with. America’s criminal justice system extends punishment far past someone’s release date, ensuring they will never get to start anew and perpetuating a cruel cycle that traps historically disenfranchised individuals within its jaws.

This system thrives off of fear. It capitalizes on our fear of the “other,” dehumanizing criminals in an attempt to divide us and justify harsher treatment within penitentiaries. Incarcerated individuals have their identities flattened, defined by their crimes and demonized by sensationalized depictions of them in the media. Murder accounts for just 2% of all crimes in urban areas and a mere 0.1% in the suburbs, yet murder cases take up 36% of the air time among all crimes reported on the TV news, leading viewers to believe prison is filled with killers when in reality it is populated largely by people convicted on nonviolent, often drug-related charges. Dramatic depictions of prisons in TV shows represent prisoners as violent, 2D characters who only serve to be apprehended by the hero cop or detective to then be cast aside, the circumstances surrounding their crime and aftermath of their conviction ignored. This exaggeration is entertaining for viewers and profitable for showrunners, and it largely targets blacks and Latinos. Fiction and reality tend to blend together in viewers’ minds, and these graphic and largely untrue depictions of prisoners help to justify increasingly harsh forms of punishment and policing.[xvi] 

This rhetoric is pervasive and dangerous. We all tend to have compassion for our fellow human beings, but that fades as we are led to no longer see some as human beings. Criminals are reduced to their crimes—a single instant of their life defines them, obscuring any past hardships or future achievements, crushing their entire life story into one bad decision. I would kill for my kids, said by many parents and with great sincerity behind the words, can also take the form of, I would steal for my kids. I would cash a fraudulent check for my kids. I would fake my address to ensure that my kids get a good education. What separates a criminal from a noncriminal, the inside of a prison from the world beyond? Maybe it is just that instant when a would turns into a did, when someone is pushed to crime by the overwhelming desperation of their circumstances. It is easier to lock up a “murderer” than someone with a name. It is easier to lock up a “burglar” than a single father, an “illegal alien” (a phrase dehumanizing in the bluntest way) than a mother fleeing gang violence. No one’s story is one night, one instant, one mistake. To quote Bryan Stevenson, a lawyer for death row inmates and author of Just Mercy, “We are not the worst thing we’ve ever done.”[xvii] Imagine your life depicted in a series of polaroids, stacks upon stacks of snapshots of everything action, every decision from which your life is constructed. What if someone were to pick up a single polaroid, or even a handful, and formulate their entire opinion of you based on what they see? We need to remind ourselves that someone’s complexity cannot possibly be reduced to a single label, and we must work to dismantle a system that uses such a small factor to determine how someone will live out the rest of their life.

There must be a better way to operate prisons—and in fact, there are proven ways to treat prisoners that benefit both them and greater society. Norway’s prison system, for example, is seen as a pinnacle of prison design and stands in sharp contrast to that of the US—large windows instead of bleak walls and stacks of books instead of empty exercise yards. With an incarceration rate nine times lower than that of the US and a 20% recidivism rate (the number of people who reoffend) compared to America’s 76.6%, Norway focuses on restorative justice, which aims to repair the harm caused by a crime rather than punish prisoners. Norwegian prisons maintain a sense of normalcy with the outside world, which is thought to better help prisoners reintegrate into society. Relinquishing of freedom is seen as punishment enough, and rehabilitation is emphasized. This alternative way of organizing prisons stems from the determination to see the good in people, recognizing their potential to improve and giving them the tools with which they can do so.[xviii]

Though it is unlikely the US will be switching to the Norwegian model anytime soon, many programs already implemented in American prisons have yielded promising results. Education programs, for example, have been shown to reduce recidivism rates across the board—in Indiana, by as much as 44% for those who complete college degrees during their time incarcerated than those who do not. The Correctional Association of New York commends educational programs for “providing an incentive for good behavior; producing mature, well-spoken leaders who have a calming influence on other [incarcerated people] and on correction officers; and communicating the message that society has sufficient respect for the human potential of incarcerated people.” Educational programs work by framing prisoners as students with the ability to learn and improve, allowing inmates to take advantage of the opportunities provided and do just that.[xix]

Perhaps a less conventional though also highly effective approach takes the form of inmate arts programs. Feedback from programs in Florida and California reported that participating inmates gained a greater sense of self and accomplishment, as well as benefits from working as part of a team that could be applied life after release; recidivism rates also reduced and inmates enrolled in the program recorded considerably fewer disciplinary actions. Inmates in Risdon Prison’s Create program in Australia reported that they felt a sense of belonging, enjoyed having a place to explore ideas and do something with a purpose, and developed a relationship with the outside world and authority figures that was not hierarchical. They felt more human, more uplifted, and more understood, thriving in an environment that afforded them the dignity they deserve.[xx]

There are a wide range of rehabilitation programs employed in prisons across the world, but the one thing they all have in common is that they recognize the shared humanity and redeemability of prisoners, concepts that are all too often lost amidst prisoners’ violent portrayal in society. Perhaps this shared humanity is most evident in the small moments: another hour spent in the library analyzing Macbeth, just like a high school student two miles down the road; working through a traumatic experience with a counselor who also treats patients from the “outside world”; a step taken back to admire a mural in the same way as an art student at the local community college. The biggest mistake we make when discussing the prison industrial complex and its devastating grip on the lives of millions of Americans is forgetting that each and every person behind bars has so much more to give to society, their loved ones, and themselves. 

I propose venturing to an unusual place to find inspiration with which to proceed when reforming our prisons. The Good Place, a TV show that explores ideas of morality and humanity as it follows four people along their journey through the afterlife, concludes with the optimistic sentiment that humans have the extraordinary capacity to better themselves when given the means to do so. Two quotes from one character, Michael, are worth sharing:
“The point is, people improve when they get external love and support. How can we hold it against them when they don’t?”[xxi] 
“What matters isn’t if people are good or bad. What matters is if they’re trying to be better today than they were yesterday. You asked me where my hope comes from? That’s my answer.”[xxii] 
There is a fundamental disconnect between how the prison system operates and the incredible tenacity, compassion, and goodness of human beings. People want to make an honest living; they want to provide for their families and be good citizens and help others succeed. But people sometimes make bad decisions, especially when put into situations that are neither fair nor equally distributed. The American prison system does not try to correct these decisions or the inequities that influence them, instead aiming to punish people brutally and without relent.

This may all seem overwhelming: the scale of the problem, the sheer number of individuals crushed beneath its weight, the vise grip it has on our society. But America’s prison system cannot be allowed to continue in its current state, with its appalling treatment of those trapped within it. Prisoners are people, people with families and struggles and a past, but most importantly people with a future yet unwritten; we have the power to determine whether someone reaches their full potential in the future by pushing for reform here and now. Change must happen, but change can be slow, imperceptible, taking form in the smallest of ways yet making the most incredible of differences. Change begins within us, with how we choose to view others, listen, extend forgiveness. We are all human beings, yet we let a wall separate who we think is deserving of love and respect from who we allow to be cast aside. Humanity is strengthened by cooperation, made all the more complete by the shared traits we can recognize and defend within each other. Chidi Anagonye, another character from The Good Place, puts this thought most beautifully: “I argue that we choose to be good because of our bonds with other people and our innate desire to treat them with dignity. Simply put, we are not in this alone.”[xxiii] We must fulfill our responsibilities to our fellow humans, on both sides of the bars, to find the strength to empathize and understand, to forgive and heal. Only then can we truly pursue a world that is more just, a society that sees the potential in all its members, filled with citizens who are connected through their shared desire to be better for each other and who give others the opportunity to become better for them."
"As a Black girl, my hair is in a constant state of metamorphosis. I’ve been around the proverbial block; I’ve sported butt-length box braids, passion twists suitable for a vacation to the Dominican Republic, every wig in the Beauty Supply Store, twist-outs, braid-outs, all of it. But the hairstyle that is the most polarizing, most feared, and most misunderstood, (and, let’s face it: the most fun) is the afro. When I wear my afro, the cashier at Whole Foods will say “Right on!” or my classmate will throw up a Black Power fist in solidarity. Some passersby veer out of the way of my towering tresses. Some grip their wallets until their knuckles are white. Most just stare. 
            So what is the afro? Is it a marker of old-school blackness? A statement? The preferred ‘do’ of revolutionaries, the salon-equivalent of a bloody grenade? Or is it just African hair in its natural state, unassuming and uninhibited? 
            It took me seventeen years to understand why something as trivial as my hair made people trip all over themselves or say the wrong thing. What I had to realize was that my hair wasn't trivial at all.  
            I had to realize that afro-textured hair, as we have it now, was only made possible through three revolutions. Four, if you include my own.
           New Orleans in the 1700s is a melting pot of French and Spanish settlers, as well as people of African, Caribbean, and Creole descent. The air is thick with paprika, sun-dried cayenne peppers, and expensive rouge. Non-European women, dragged from their motherlands and catapulted into the colonial world, try as hard as they can to preserve their complex culture, including the elaborate hairstyles of their ancestors. They braid close to the scalp, wear a variety of buns and top-knots, and adorn their strands with gemstones, beads, and petals (Callender). White women, irked by these showy displays, petition Governor Esteban Rodriquez Miro to do something about the “exotic”, dark-skinned temptresses. 
            In response, Governor Miro enacts the “Tignon Laws”, which require women of color to wear a tignon, a type of head wrap, to cover their hair. Not only did it shield “unseemly” Black hair from wandering eyes, but it also indicated social status and race (Callender). 
            Black women, ever inventive, began to wear flamboyant tignons with rich colors that drew even greater attention (Callender). Discomfort with Black hair and Blackness as a whole has existed since the beginning, and since the beginning, we have rebelled.
            In the glare and scorn of the public, my ancestors danced. 
            The 1960s gave way to the second revolution: the Civil Rights Movement. Activists preached the reclamation of every aspect of Blackness, which manifested into the Black Power and Black is Beautiful movements (Gabbara). Self-acceptance, self-actualization, self-governance. Self-love, without apologies, without shame--this was the unifying theme of the multifaceted, often convoluted crusade for civil rights. 
             Being ‘Black and Proud’ meant being proud of everything to do with Black identity, and at the core of this reclamation was hair. Before the sixties, afro-textured hair was an unrelenting river of shame for African American women. Many used toxic hair-straightening compounds like lye and extreme heat to avoid being branded “nappy”, “wooly”, or “kinky”. But suddenly, college students across the country roamed campus with sky-high afros. Older Black omen let their coils loose. The hot combs, relaxers, and Eurocentric beauty ideals had, momentarily, lost their hold in the Black community. According to Chad Dion Lassiter, president of the Black Men at Penn School of Social Work, Inc., afros were an “outward sign of their frustration toward Dr. Martin Luther King Jr.’s nonviolent philosophy … the Afro was Black beauty personified without White validation, and it did not care about critics” (Gabbara). For activists, the afro was never “just a hairstyle”; it was an instrument used, in conjunction with their writings, speeches, and protests, to dismantle the system of white supremacy. 
            To wear naturally afro-textured hair was activism. 
            With the seventies came another revolution, a different revolution. The afro, which once marked its wearers as ruthless “radicals” and militaristic ""Marxists”, was now worn to discotheques across the country. The Supremes wore afros, the Jackson Five wore afros. The Blaxploitation films of the seventies also played a key role in bringing Black experiences (and hair) into the mainstream (Gabbara). Movies such as Foxy Brown and Coffy, which involved “bad-ass” Black characters usually involved in crime, prostitution, and gambling, pummeled urban audiences with images of African Americans in their “natural” state. The afro became an accessory. 
           The fourth revolution happened the summer before fifth grade. My mother asked me if I wanted to go natural. I said yes, because it meant that I wouldn’t have to use stinky, painful relaxers anymore. I was ready for slippery, silky, flowing curls, like Zendaya's or Yara Shahidi's. I couldn’t wait.  
           As it happens, going natural was not as easy as we thought it would be. My hair, half-relaxed, half-natural, and straight from the motherland, would not yield to wide-tooth combs, extra-strength conditioner, or even sheer force. Washing it took upwards of three hours and blow-drying it made my arms ache. I spent countless afternoons in front of the mirror, face pruned from shampoo or tears or a mixture of both, attacking the kinks on my head. 
          The only person who hated my hair more than I did was my mother. This wasn’t what we asked for. It wasn’t slippery or silky--it was angry. Neglected.  
          Anyone who has ever been a teenager in America, especially a Black teenager, knows the acute melancholy that comes with not liking yourself. I'm not talking about just hating the way your hair falls or the slant of your eyebrows, but looking down at your sneakers in the hallway so you won’t have to meet anyone’s curious gaze, or walking with your shoulders rounded forward like a makeshift shield, or simply trying to not take up so much space. 
          It took years before I was able to wear my afro out in public. There was no method to the madness, no book out there titled how to make peace with your knotty, kinky hair. So I did my own research. I absorbed the stories of the women who came before me. They taught me to keep my chin up. To square my shoulders. To dance and flirt in the face of contempt. 
          And when I did let my coils loose, it was like a bloody, arduous war had ended and I had come out the victor. For the first time in my life, I felt like those New Orleans women--stunning, funky, interesting. I took up space. And I didn’t apologize for it. 
          Not once."
"At the Berlin Conference of 1884-1885, all the major European powers (England, France, Belgium, Germany, Portugal, and Italy) convened and “decided” how they would divide up the continent of Africa among themselves; the Congo was “made” the personal property of King Leopold II of Belgium. Over the next decades, Leopold and his fellow Europeans would mercilessly extract resources and people from Africa, but—astoundingly—under the guise of a humanitarian mission: These imperialists professed a desire to “rescue” the “savages” from their ways, to bring “civilization,” and to spread Christianity to the “dark continent.” In his novel Heart of Darkness, Joseph Conrad, a staunch anti-imperialist, offers a critical examination of the discourses underpinning Africa’s colonial exploitation. The protagonist of the novel is Charlie Marlow, who, for most of the novel, is himself narrating the tale of his experiences in the Congo to his fellow crew-mates on a boat in London and, simultaneously, to the reader. Conrad argues that Marlow soon recognizes that European discourses of “progress” simply  aim to obscure the moral horrors taking place under colonial rule. However, Conrad suggests that, when Marlow tries to comprehend these horrors by capturing them in a language-based framework, he fails, and this failure leaves him morally and epistemologically paralyzed. 

As he describes his arrival in the Congo, Marlow recalls that he soon learned that European colonists’ discourse of “progress” and civilization obscured the true nature of their deeds; however, he implies that, bereft of any explanatory discourse to help him process what he was seeing, he entered—and remains in—a state of psychological and epistemological chaos. Marlow states that, when he saw Africans being forced to work on the company station, the colonists told him they were “criminals” and “enemies” to whom the “outraged law… had come”; he stepped into the “shade”—where, seeing a laborer who was a “dying… shadows of disease and starvation,” he gave him a “biscui[t]”—and, horrified stepped out of the shade and into the presence of a company accountant with “white cuffs” (18-21). By claiming that they were applying the “law” to “criminals,” Marlow implies, the colonists were suggesting that they were on some heroic mission to introduce justice and “progress” to a land of “savages.” But Marlow’s narratorial juxtaposition of the noble language of “law” with the realities he had observed—“dying,” “disease,” and “starvation”—suggests that he was able to see the disjuncture between the colonists’ elevated narratives and the rapaciousness of their actions. He further emphasizes his recognition of this disjuncture by suggesting that he identified the “shade” as the locus of true knowledge, which implies that he—in a disruption of traditional Western black-white binaries—could see that the light and “white[ness]” of the colonists and their discourses represented not enlightenment but concealment. However, the fact that Marlow instinctively stepped out of the “shade” and into the “light” suggests that he felt uncomfortable with this new knowledge and would rather have avoided its disruptive implications. After all, colonial discourses of “progress” and civilization—to which Marlow had presumably been exposed his entire life—might have provided him with the (false) comfort of a language-based framework through which to capture and understand the nature of what he was witnessing. As the narrator, Marlow reveals that—once stripped of the illusions underpinning colonial narratives—he was left to confront the radical horror and otherness of colonial Africa without any explanatory framework to guide him: When he saw the Congolese laborer “dying,” Marlow’s narration suggests, he simply had no means to process what he was seeing, and could therefore do nothing more than give him a “biscui[t].” One could argue that, even at the moment of narration, Marlow remains without any means of comprehending or explaining what he saw in the Congo. That is why he is recounting his experiences—in the hopes of establishing some framework to parse them. 

Marlow, through the very act of narration, tries to forge a language-based discourse that recognizes and explains the radical horrors he has witnessed, but the layered nature of these horrors—their occupation of the moral and psychological realms as well as that of the physically observable—places them beyond the reach of language, which is, as Marlow himself indicates, fundamentally superficial. Marlow asserts that, when his boat was being “shot” at with “arrows,” he was surrounded by a “white fog” that put him in a “trance,” after which the helmsman was “shot” and gave an “extraordinary, profound” look before dying; at a different point, Marlow defeatedly tells his audience that he feels like he is trying to “tell… a dream… [of] being captured by the incredible” (48, 56, 32). Marlow’s use of the words “dream” and “incredible” evoke the image of experiences that lie beneath—that lie beyond—the mundane realm of exterior physical activity and perception. Therefore, Marlow’s suggestion that “tell[ing]” and language are incapable of capturing such inner “dream[s]” and “incredible” experiences suggests that he recognizes that words are intrinsically superficial—they are perhaps capable of recording “exterior” humans actions but not of documenting the inner moral and psychological experiences that dwell beneath them. Yet, paradoxically, the very horrific experiences Marlow is trying to document in words affect him primarily on a moral and psychological level—as opposed to the level of mundane surface-level physical perception. For example, in the scene where Marlow describes being shot at, his retrospective use of the words “trance” and “white fog” suggests that he had entered a mode of physical perception that was not really perception at all—that he was perceiving this event primarily through the lens of his own inner psychology and inner moral state. Yet when Marlow tries to describe his response to the helmsman’s death on a psychological level, all he can do is to list hollow, meaningless adjectives—“extraordinary,” “profound”—that ultimately skirt around, and reveal nothing about, what he was undergoing or feeling. Marlow's words are unable, it seems, to cut to the heart of the moral and psychological horrors he has experienced in the ""heart of darkness.” 

Marlow’s narration implies that the failure of his explanatory discourse throws him into a state of moral paralysis: Without any framework to help him assess the horrors he has seen, Marlow does little more than passively relay impressions from his own experiences—unable to decode or assess them for himself—in the vain hope that his audience (which includes the reader) might be able to penetrate those impressions for him. For example, Marlow describes the scene with the heads on the pikes largely in terms of an avalanche of sensory impressions, from the “shrunken dry lips” to the “round knobs” to the “narrow white line of the teeth”; at another point in his story, Marlow asks his crew-mates (and the reader), “Do you see the story? Do you see anything at all?” before pointing out that they, like all human beings, spend their entire lives dwelling on the “surface” of existence without considering the moral and psychological “realit[ies]” beneath (71-72, 32-33, 42). In the scene with the heads, Marlow’s exhaustively lists out his impressions but dedicates little time to analyzing them, suggesting that—lacking an explanatory framework to help him assess such impressions—he is unable to decode them himself. Therefore, one could interpret the questions Marlow asks of his audience—“Do you see the story? Do you see anything at all?”—as representing not Marlow’s impatience with incompetent listeners, but rather a plea for help: He is asking these questions, one could argue, because he cannot “see anything at all” himself—he has no explanatory discourse to light his path—and simply hopes that someone else might be able to help him “see,” to pinpoint the deeper moral implications of his experiences for him. But this is a futile hope. As Marlow himself recognizes, all people (including himself)—trapped in a system of language designed to document surface-level physical events—merely occupy the “surface” of reality without ever probing what lies beneath. If it is indeed language itself that cannot penetrate the moral and psychological realms—and, in particular, moral and psychological horrors—then perhaps there is no one who can create the discourse Marlow so badly needs. 

In Heart of Darkness, Joseph Conrad implies that the nature and structure of human language itself are well-suited to the rhetoric of colonialism. Human language, in Conrad’s conception, is a superficial mode of communication—it forecloses the possibility of expressing complex, multilayered ideas, and it therefore lends itself to discourses that simplify and that are simplistic. And as Conrad suggests, the discourses of imperialists—chauvinistic words that delineate a forceful, arbitrary binary between “savage” and “civilized”—certainly fit that definition. "
" Since March of 2020, the COVID-19 pandemic has disrupted every household in America in some capacity, but what most fail to consider is the other plague that is simultaneously disrupting millions of American households: food insecurity. As of 2009, more than 50 million people in the United States lived in a household classified as food insecure. Today, food insecurity is one of the most destructive nutrition-related public health issues in the U.S.[i] The United States of America is a country that had a 1.4-billion-pound dairy surplus in 2017 (and proceeded to dump millions of pounds of milk onto fields as an effect), and a country that continues to discard billions of pounds of food each year[ii], so how is it that millions of Americans are still going hungry in a country that has an overabundance of food? To understand modern food insecurity, one must first understand what it is and where it stemmed from. In simple terms, food insecurity is hunger; it is the lack of access to nutritious and affordable food. Food insecurity in America can be traced back for centuries; Americans have been food-insecure since the birth of the nation in the 18th century. Throughout the Gilded Age, poor working conditions, the lack of government regulation, and low salaries caused many workers to be food insecure as they did not have the means to provide meals for themselves and their families. During the Financial Panic of 1893 and the Great Depression in the late 1920s, food insecurity rates rose once again due to the high reported rates of poverty and unemployment. Modern food insecurity can be traced to Depression farms in the 1930s and New Deal policies. Redlining in American cities and communities resulted in the emergence of the grocery gap: a phenomenon existing in impoverished communities where it is easier to buy high-calorie, processed foods than fresh fruits and vegetables[iii]. Over time, these disparities only grew, and due to COVID-19, they continue to rise once again. Today, neighboring communities face completely different obstacles; one may be a food oasis and the other, a food apartheid (""apartheid"" refers to the idea that food insecurity is primarily a result of failed policy and is not naturally occurring). Food security is a fundamental right that every American and every human deserves. No human should live in fear that they do not have the means to provide adequate meals for their family. No human should have to sacrifice their own meals to provide for their children. No human should be denied their fundamental rights.
           The existing disparities between food insecurity rates and access to food in different communities are massive, and it is only growing larger. Imagine a city housing two communities, only miles apart, which face entirely different living circumstances. One community is the home to many migrant farmer families living food insecure. Some families can only afford to eat one meal a day, and many have developed diabetes, high blood pressure, and asthma as a result. Most do not have access to health insurance, and all medical expenses must be paid out of pocket. A few miles away from here, another community has access to an abundance of farmer's markets and grocery stores. Families living here have more than enough food to eat, and there is no lack of fresh fruits and vegetables.[iv] This is the disparity of food insecurity between communities in Santa Barbara, California, and many other American cities and towns. This disparity exists as a result of the relationship between food insecurity and income. The considerable price difference between unhealthy and healthy food and high calorie processed foods and fresh fruits and vegetables is unaffordable for many. Americans must sacrifice their health either by eating very few nutritious meals on an irregular basis or unhealthy, high caloric meals. It is also important to note the disparities between food insecure people in terms of race. More than 21% of all black people were food insecure or lived on the edge of hunger in 1999, and 20.8% of Hispanics, in contrast with only 7% of non-Hispanic white people, were reported to be food insecure.[v] Low-income and non-white neighborhoods have more fast-food restaurants than their counterparts. As a consequence of this grocery gap, low-income minority groups tend to consume fewer fruits and vegetables and have higher rates of obesity than their counterparts. These disparities between different groups of people concerning food insecurity can cause problems of their own. Food is essential in understanding the construction of personal and social identities, perpetuating racial stereotypes. Throughout history, black figures on food such as Aunt Jemima have become corporate trademarks, portrayed as an offensive and subservient figure. These stereotypes of the black community have been harmful, and in some cases, have contributed to the disparities around today among black and white community's food security rates[vi]. To combat these disparities, national and local policies have since been created to address food insecurity and target geographic and economic access to food. Through federal national benefits (such as S.N.A.P.) and tax incentives (such as the Healthy Food Financing Initiative), food has become more available to these target communities[vii]. However, with food insecurity rates on the rise as a result of the COVID-19 pandemic, these federal programs cannot address all aspects of food insecurity or reach all that are food insecure. Food environment, the availability of food stores in a community, also affects the diet of low-income Americans, and few legislations and programs combat unjust food environments. In urban and rural areas, where food insecurity rates tend to be highest, smaller food stores, convenience stores, and fast-food restaurants are in excess, while supermarkets and farmers markets dominate suburban communities[viii]. The disparities between food insecure and food-secure communities are significant, and they present pressing issues. They complicate the meaning of citizenship, as these disparities between communities complicate the American ideal of equality. When some Americans live in extravagance while others live insufficiently, the idea of equality in terms of American citizenship becomes convoluted.
           The existence of the hunger-obesity paradox complicates how American policies address hunger. In 2010, the American community with the highest reported food insecurity rates, the South Bronx district of New York City, also had the city's highest obesity rates[ix]. This is the hunger-obesity paradox: the idea that both hunger and obesity coexist in many Americans. Due to industrialization and urbanization in the 19th and 20th centuries, many Americans migrated from rural America to American suburbs and cities. As Americans mass-migrated, their knowledge of growing and preparing fresh, healthy foods diminished. During the post-World War II era, America began to produce more highly processed foods due to its newfound popularity. Canned, processed, shelf-stable, and microwavable meals would become common in American households following the 1950s[x]. As the types of foods available for preparation changed, so did the average American diet. For many low-income families, highly processed foods became the only option for supplying enough food for themselves. Under these difficult circumstances is where the hunger-obesity paradox exists. In the South Bronx district of New York City, the Health Bucks program encourages people to spend their S.N.A.P. food benefits at farmer's markets where every $5 spent results in an additional $2 benefit[xi]. Although programs like this can help combat food insecurity, they are not sufficient enough. It is unreasonable to assume that someone living in poverty would have access to transportation or the time to get to a farmer's market, as they are most commonly located in areas that are inaccessible to those living in food environments similar to the South Bronx. Like the Health Bucks program, food justice legislation, which fails to address food availability in low-income neighborhoods, is ineffective. It is not until America changes their perception of hunger that effective policies can be enacted. Not only should citizens have the right to be free from hunger, but the right to adequate access to nutritional, affordable foods. Legislation must combine efforts to increase accessibility and affordability to truly be effective. 
           The severe consequences of food insecurity complicate American ideas of equality through divisions in the existing health community. Pregnant women who are food insecure are at higher risk of congenital disabilities. Food insecure households are at higher risks of lower nutrition intakes, anemia, mental health issues, being hospitalized, and poorer general health. Due to the high presence of the hunger-obesity paradox, food-insecure Americans are more likely to have conditions such as Type 2 Diabetes and high blood pressure[xii]. A 2009 C.D.C. study estimated that annual obesity-care expenses in 2008 were at $147 billion, as much as 9 percent of all medical costs, and nearly doubled the 1998 estimate[xiii]. Hunger has always increased the likelihood to suffer from medical conditions, which causes food insecurity to impact other aspects of a person's life, such as their ability to work. In an effort to fight hunger, food security and health security have been added to the list of national security issues, resulting in more awareness and promoting the implications of new programs to supply healthy food and healthcare to those who do not have access to it[xiv]. Despite these efforts, it is essential to note that some food and health campaigns and foodservice providers are profitable businesses in disguise. Many American food banks marketed as humanitarian non-profits are linked to big for-profit businesses because of their existing partnerships between private food aid and large corporations. These sorts of schemes, sometimes unintentionally, promote food security as a biopolitical project rather than a national health crisis with severe consequences[xv]. The effects of food insecurity are real and pressing; until all food-insecure citizens have access to the care they need to combat existing health conditions, this crisis cannot be overcome. Health care is a crucial aspect of fighting existing food insecurity. 
           Access to an adequate standard of living is a fundamental human right. When living food insecure, one does not have access to this right. When a citizen does not have enough to eat, they are at a much greater risk to experience health issues, which can lead to an inability to work, resulting in decreased income. Food insecurity is cyclical in nature, and those who are living food insecure are not given the right resources to break out of this cycle for themselves and their children. America is a nation that prides itself on being the land of the free, a country where everyone is equal. The reality that the existing disparities between communities allows one to continually turn a blind eye to the other is truly paradoxical. There is a lack of accessible and affordable food in America: this must be acknowledged before it can be resolved. This is because of the disparities between American communities that jeopardize the American values of equality. Americans must begin to acknowledge the existing disparities between communities, change their perceptions on hunger, recognize the risks and severity of food insecurity, and acknowledge that food security is a right that every American should have access to. Only then can America begin to develop effective legislation that tackles all aspects of food insecurity to alleviate and remediate the problem."
"In 2009 following the release of Disney’s Princess and the Frog, there was a national salmonella outbreak because children began kissing frogs. We have to give them some credit—it doesn’t hold a candle to consuming tide pods—but still, we can’t help but marvel at their impressionability. The funny thing is, however, it’s not just children who are so easily changed by what they see in media: According to National Geographic, the movie Jaws is almost single-handedly responsible for Americans’ fear of sharks. NPR notes that the release of The Hunger Games caused national participation in archery to almost double. Study after study indicates that what you see on a screen can shape how you view the government, your career choices, your relationships, and your sense of identity. Film and TV have a profound impact on how you see the world.

Because of the effect media has on us, it is crucial that we remember that the concepts behind films and movies aren’t generated in a vacuum; they’re stories, so they depend on the perspective of the storyteller. While actors do the delivering, the people who actually shape the story are the ones sitting in the director’s chair or in the writers’ room. We find that even as the screen gains color and variation, the people behind the scenes often remain startlingly undiverse. Our storytellers are disproportionately white and male; while some of my favorite films are by brilliant writers who are white men, it’s important to note that scripting is storytelling, and storytelling benefits from diversity in perspective. Minority representation in directors, producers, and editors along with diverse writers rooms can provide this in film and TV, respectively, but that level of representation remains rare in the industry. Without diverse storytellers, we are often limited to a single perspective, and the consequence is one-dimensional representation of minorities that hurts both the actors and the audience as it prevents minority stories from circulating.

A primary example of lacking representation is how members of certain races begin filling “token” roles in the story that often play into stereotypes. For example, there’s a tendency for TV shows to emasculate Asian men, like Raj from Big Bang Theory or Han from Two Broke Girls, both of whom demonstrate feminine tendencies for comedic effect. Similarly, comedic duo Key and Peele make light of how the role they always end up auditioning for is that of “The Black Best Friend,” a stock character that rounds out a group of white friends and whose character depth is limited to sassy one-liners and undying support for the main character. There’s also a painfully obvious tendency for action and horror movies to kill off the colored characters first, which The Walking Dead was particularly guilty of in its treatment of black male characters. The spotlight is almost always on white characters in mass media, while other races are relegated to the background, underdeveloped, or disposable.

This imbalance isn’t necessarily about the quantity of representation—it’s not about ticking off a box and patting oneself on the back: “I stuck a black guy in. Yay diversity!” It’s about quality, giving minority characters the complex storylines they deserve. We see this most clearly with women in Hollywood, who aren’t missing quantitative representation, but whose portrayals often lack quality; barring those made for all-female audiences, movies seldom feature female protagonists. Women are also highly sexualized in film, with the APA cautioning that 40% of the young women to appear in Hollywood's top films over the last decade were “scantily clad or naked”. How does this impact how society sees women? How young girls are encouraged to see ourselves? Issa Rae’s thin smile as she announced an all-male list of 2020 Oscar nominees for best director speaks volumes; female directors lack distribution, and thus the media that reaches the most people overwhelmingly features female characters that act as love interests and serve as the protagonist’s “prize”. Just how many times have we seen a storyline where a male hero undergoes some trial—anywhere from saving the world to a bit of character development—, comes out victorious, and is expected to “get the girl” by default? Female characters’ plotlines revolve around those of male characters all too often.

It’s not right that talented actors are being limited, that minority viewers can’t see themselves, and our society doesn’t have the benefit of seeing different people’s stories. That’s why movies like Wonder Woman, Black Panther, and Crazy Rich Asians are lauded as the triumphs they are—they’re making waves in putting fully realized minority characters onto our screens by putting minorities in the director’s chair. Several newer TV shows also present a diverse, complex cast; Black Mirror, for example, shows us that anyone, regardless of race, gender, or sexual orientation, can be brutally tortured by futuristic technology. The Good Place, Jane the Virgin, and many other series popular with Gen Z are similarly good at handling diversity, and accordingly, all of these shows boast diverse writers' rooms and producers. When minorities begin to take on roles behind the scenes, we see those stories represented on the screen. 

I can say firsthand that this element can mean everything to viewers; growing up, I wanted to see a protagonist I could connect to, who might be telling my story; as a colored girl, I’d resigned myself to the fact that I’d seldom be able to identify with a main protagonist. The characters who formed their words the way I did or whose skin matched my own were often exaggerated caricatures, leaving me with a tight-lipped smile and a queasy feeling in the pit of my stomach each time their oddities set off the laugh track. Now, as I watch my younger sister gleefully don her Elena of Avalor Halloween costume—“She looks just like me!”—I can’t help my giddy astonishment at the growing changes.

We can continue to move in this direction by supporting minority filmmakers through paying to see their work instead of illegally downloading it, thereby allowing them to gain funding and distribution. In addition to practicing conscious consumerism, we need to open ourselves up to new stories because the stories we are exposed to are the lenses through which we see the world. Stories, whether expressed in conversation or written and visual mediums, are our means of connecting with people of different identities we couldn’t otherwise understand, and anything that precious must be shared."
"A few hundred miles away from the bustling capital of South Korea, one finds a small island replete with lush mountains, fields of cosmos flowers, and vibrant bodies of water. Here, Korean city dwellers seek an escape from their demanding lives, and tourists from all corners of Asia are drawn to the natural landscape. While a flourishing tourism industry gives Jeju Island an appreciable stake in the Korean economy, it has otherwise been considered detached from the mainland’s politics. Throughout the turmoil of a presidential impeachment and widespread protests against government corruption, the small population of Jeju Island did not exert much political sway.
           In early 2018, however, the name of the island decorated headlines for something much different from its natural wonders. Some 500 asylum seekers from Yemen, Syria, and Egypt poured into Jeju Island, constituting an influx that dwarfed the slow trickle of refugees typically seen in South Korea. These refugees -- the majority of them Yemeni -- were fleeing a catastrophic civil war that has claimed more than 60,000 lives to date. Many of them had initially sought refuge in Malaysia, but upon learning that Malaysia was not a signatory to the United Nations Refugee Convention, took a budget-airline flight to South Korea in search of better living conditions (Kwon). This was made possible by a 30-day visa-free policy devised by the Korean government to attract foreign tourists to Jeju Island. For these refugees, the end of the visa-free period marked South Korea’s transition from a sanctuary to a hostile foreign land.
           A bewildered public looked to European refugee crises to predict the future of their country -- this, obviously, was not a source of optimism. In the coming weeks, a resounding outcry against the Middle Eastern refugees was heard in urban areas of the peninsula. Thousands of protesters took to the streets demanding that the refugees be deported immediately; 700,000 more signed a petition imploring the president to put the deportation into effect[1]. It quickly became the most popular petition on the online platform offered by the President’s Office, surpassing pleas to decriminalize early-stage abortion or to take measures against child pornography. In response, the government proposed an action plan. Parts of it, such as the hiring of translators at the Jeju Immigration Office, were inoffensive; other parts simply gave into the public’s apprehension, adding twelve Middle Eastern and African nations to a list of nations excluded from the visa exemption policy.
           For a country that was devastated by civil war and poverty not too many decades ago, why was it unimaginable that others would be seeking respite from the same? Clearly, factors other than a simple lack of empathy were at play. Political context complicates this comparison; South Korea has only recently begun to embrace multiculturalism as an alternative to the strong, homogeneous national identity that had been set in stone following 35 years of colonial rule. Though Koreans were initially reluctant to welcome any foreigners, the country’s diplomatic ties with the U.S. naturally exposed them to Western cultures that were viewed as admirable rather than alien.
           Despite the steady integration of Western and East Asian immigrants into Korean society, South Korea has turned a cold shoulder to refugees ever since it settled into economic stability. In 1992, the country became a signatory to the UN Refugee Convention, pledging to play their part in protecting refugees whose home countries pose threats to their life or freedom. In 2012, Korea even adopted an official refugee law, but it seems that their consequent humanitarian duties have been all but abandoned. For the past two decades, the Korean government accepted only 42,000 asylum seeking applications; of those, 849 people (about two percent) were granted refugee status (Ock). Those permitted to stay have formed enclaves that insulate them from full cultural or linguistic integration. Their job prospects are severely limited -- refugees attest to working for weeks without being paid, working 18-20 hours a day, and even being physically assaulted by employers (Ghani). While the recent Yemeni refugee crisis was a rude awakening to Korea's mishandling of its international obligations, it is merely a symptom of a decades-long problem, waiting to surface.
           On face, it is unclear what motivates such intense xenophobia in a highly educated, well-to-do country that could comfortably accommodate a few hundred refugees. A study by Hankook Ilbo (a major Korean news outlet) investigated the weight of variables such as age, gender, and monthly income on Koreans’ attitude toward Yemeni refugees (Lee and Park). The trends gathered in this survey did not paint a clear picture. Respondents in their 40s were less opposed to refugee acceptance than those in their 20s and 30s, despite younger people being the driving force behind Korea’s shift to liberalism. Women objected more strongly than men, and monthly income had no discernible effect on people’s opinions. Put together, these results indicate that no one personal attribute explains away the nationwide uproar, so plainly visible on the streets.
           We then turn, not to dissecting each individual’s position on refugee issues, but to examining the basis of Korean society’s general attitude toward refugees. Owing to its history of frequent but inconclusive foreign invasions, Korea is an incredibly ethnically homogeneous nation. 97.7% of the population is ethnically Korean, and other ethnicities barely comprise 1% of the population. A robust body of scholarship explains that the genealogy of Korean nationalism began at the end of the 19th century, when Japanese and Western powers descended on the peninsula (Shin). Politicians of the severely weakened Joseon Dynasty clung on to nationalism as a form of resistance. This brand of nationalism particularly focused on ethnic uniformity by design, directly contesting imperial Japan’s narrative that Japanese and Korean people shared a common ancestry. The political sentiment that emerged was one that reified the nation and its native-born people, rather than its shared civic ideals.
           While an abstract ideology dating back to the 19th century may seem removed from the 2018 refugee crisis, the power that ethnic nationalism still wields over Korean society cannot be underestimated. A principle that is, in most countries, associated with right wing populism, instead spans both ends of the political spectrum -- Korean-ness is a prerequisite for participation in society (Lee). It should not come as a surprise, then, that one of the most common slogans seen at protests was “the people of Korea come first.” Refugees, universally portrayed as vulnerable and at a loss for national identity, were seen as taking away from the resources, land, and pride that rightfully belonged to Koreans. Similar responses have been seen in other nations; in Norway, for example, announcements of refugee acceptance (in numbers that have never exceeded 10,000) have also been couched in nationalistic language. Sylvi Listhaug, the Norwegian immigration minister, emphasised that Norway is faced with “a big challenge now to integrate those with permission to stay in Norway to make sure they respect Norwegian values” (Nelson). Her words illuminate a clear parallel to South Korea: both developed nations with an uncompromising sense of national pride, reluctant to open their doors to strangers needing refuge.
           Even outside the scope of refugee issues, the rejection and disenfranchisement of minorities is not a uniquely Korean phenomenon. Analogous practices of cultural and legal gatekeeping can be found throughout history. A particularly troubling example lies in the progression of racism in the United States, a practice dating back to slavery in the 17th century and persisting through its cultural remnants in the modern day. In the decades following the Civil War, America saw Jim Crow laws upholding segregation and violence intended to suppress Black Americans’ political freedom. Public spaces and schools were racially segregated, Black people had no legal recourse if found guilty of crimes they did not commit, and the fear of lynching was both real and omnipresent as the Ku Klux Klan underwent a resurgence. Only in 1964 -- a full century after the 13th Amendment legally abolished slavery -- was the Civil Rights Act passed, ending the codification of Jim Crow laws. At this point, however, institutionalized racism was already deeply ingrained in both the South and the North. In 2020, it still prevails in the country’s memory.
           In particular, one consequence of the Jim Crow era stands out as comparable with the shunning of refugees in South Korea: the use of law enforcement as a systematic means for disenfranchisement. Prior to the abolition of slavery, the freedom of the Black population was taken away by the word of the law. Though the passage of new bills and civic activism apparently put an end to this pillage, overt inequality simply became subtle enough to seep through the Constitution’s loopholes. The Atlantic documentary, “Angola for Life: Rehabilitation and Reform Inside the Louisiana State Penitentiary,” draws attention to a caveat in the 13th Amendment: “Neither slavery nor indentured servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States” (Benns). This convenient exception has since been exploited endlessly by those in power, criminalizing Black people for all manner of petty misconduct and using their incarceration as justification for slavery masquerading as “prison labor.” This, according to legal scholar Michelle Alexander, effectively maintains a modern-day caste system in America (21).
           While the Korean refugee crisis is not as historically entrenched as the American racial divide, the iron-fisted treatment of refugees is headed in a dangerously similar direction. Koreans have seemingly bought into the “broken windows” theory, which claims that criminals perceive signs of minor disorder (e.g., broken windows) as indicative of a powerless government (Kelling and Wilson). In the U.S., police departments enthusiastically caught onto the idea and used it to justify “zero tolerance” policing, unsurprisingly targeting a disproportionate number of minorities. Much like broken windows, the crimes that came under attack were benign, victimless infractions; while it is true they posed some inconvenience, their criminality was mostly a figment of the public’s imagination. Using this principle, the Korean media, government, and public have jointly invented a divide between “real refugees” and “fake refugees” (Choe). Echoing white America’s criminalization of Black citizens, both out of real (albeit unfounded) racial anxiety and a desire to oppress, Koreans were quick to accuse Muslim refugees of high-profile crimes they did not commit. Some of these accusations were simply variations of prevalent Islamophobic beliefs that conflate refugees with Islamic terrorists. Others alleged that Yemeni refugees were sexually assaulting female tourists in Jeju Island. One protester summed it all up when he told the Arab News, “Refugees, including Yemenis are not only a threat to our people but also that to our future generation. Some of them are not even real refugees. Most of them are able-bodied men so they’re fake” (Sung). With a level of efficiency unprecedented in gridlocked politics, Korean Congress passed a set of laws, uncannily similar to zero tolerance laws written by past U.S. governments. The revised ‘Refugee Act’ would carry out strict background checks, crack down on visa overstays, and terminate asylum applications for anyone who had temporarily returned to their home country. Informally, local governments advised new refugees to “stay indoors as much as possible” (Bloomberg).
           Needless to say, neither the U.S. criminal justice system nor the Korean refugee crisis is close to a just resolution. However, in similar problems we may find similar solutions -- many of the reforms that U.S. racial justice activists have proposed are applicable in South Korea as well. Organizers behind campaigns against racism in policing have long advocated for the decriminalization of drug possession, a victimless crime for which Black people have been disproportionately convicted (Cooper and Thompson). Plenty of evidence favors this policy change; a statewide study in Oregon projected that decriminalizing the possession of controlled substances would reduce the number of convicted Black and Native residents by approximately 94% (Oregon Secretary of State). Across the U.S., drug-related offenses are the most common cause for arrest. Among those arrested, defendants who cannot afford legal representation or bail payment, i.e. largely poor racial minorities, are carried away to the prison industrial complex. Thus, increasing legal leniency for drug possession has frequently decorated the agendas of organizations such as the ACLU.
           Across the Pacific Ocean, refugees also struggle against the grain of the Korean legal system. The Justice Minister himself cautioned that refugees who “contravene the social order” would be deported, at the same time that he announced strict penalties for “refugee brokers” who facilitated refugees’ migration (Park). As a result, 56 out of the 500 Yemeni refugees were ordered to leave Korea, 400 were granted tenuous “humanitarian stay” permits that could be revoked upon any trivial infraction, and only two were granted refugee status (Global Detention Project). To allow refugees any chance of successful integration, the Korean government must lift the unforgiving legal standards that leave their right to stay constantly hanging in the balance. The government ought to take the advice of experienced activists in the U.S. and stop treating criminalization of the most oppressed as an antidote to social disorder. Practically, this would look like lifting travel restrictions that limit refugees’ right to move outside their port of arrival; not penalizing asylum seekers who had to rely on brokers to get to the country; discontinuing the xenophobically charged profiling of refugees who have no reason to be suspected of criminal activity.
           Furthermore, South Korea would come closer to fulfilling their humanitarian obligation by establishing programs that allocate financial, social, and legal aid to refugees -- not unlike existing welfare programs abroad and at home. In the case of the U.S., the National Immigration Forum reports that resettlement agencies assist refugees with accessing essential services such as school enrollment and applying for social security. Some financial assistance is offered as well; the State Department’s Reception and Placement (R&P) program funds resettlement agencies to finance refugees’ first 30-90 days in the U.S. Similarly in Canada, asylum seekers are entitled to healthcare, as well as housing assistance and social welfare in many cases (Cheatham). While some initial political backlash is to be expected -- those who were anti-refugees will not become pro-refugee-welfare overnight -- such reforms may ultimately lead to a change in public attitude. Whereas the continuous deportation of refugees will simply confirm Koreans’ belief that they are criminals who do not belong in the country, increased legal forgiveness and assistance will give refugees a chance to participate in society without fear of overstepping the law. Not only does this promise to dismiss the perception of criminal behavior, it will also allow refugees to become more self-sufficient, as former skilled workers will occupy higher-paying jobs than are available in the fishing and agricultural industries. In a country that places high value on individual merit and frowns upon non-nationals who apparently take without giving, refugees need room to demonstrate their virtues, both for their welfare and for Korea’s future role in the global refugee crisis.
           Between 2015 and 2017, the number of people applying for refugee status in South Korea nearly doubled. The 2018 refugee crisis was not an isolated incident, but rather a harbinger for continually increasing refugee numbers. South Korea has thus far gotten away with positioning itself away from the crises in Europe and Africa, but can shirk its responsibility only for so long. The Korean public is not entirely to blame; the ethnic homogeneity of the country and historical need for nationalism have rendered Koreans predisposed to xenophobic beliefs. But it is possible to be both patriotic and open-minded. Rather than giving into citizens’ fear-driven demands, the government must exempt refugees from punishment for petty crimes and assist their path to economic independence. Unless they reverse the course of refugee mistreatment, far down this road is a social hierarchy reminiscent of America’s prison industrial complex and systemic discrimination. The Korean government sits on the chance to join its international peers by treating refugees with kindness; they must do so before intolerance calcifies in the country’s politics."
"Few modern superhero stories have ever been as reflective, deconstructive, and deeply knowledgeable of the superhero genre as Alan Moore’s and Dave Gibbons’s masterwork, Watchmen. A DC-serialized graphic novel first released in 1986, Watchmen appealed heavily to the zeitgeist of an America under conservative leadership during the tail-end years of the Cold War and redefined the superhero genre in all its forms and permutations for decades to come. Fully describing the incredible impact of Watchmen is a gargantuan task, and attempting to further elaborate on what made it so vastly acclaimed is just as difficult. However, a good introductory starting point to Watchmen’s near-universal resonance is its alternate-universe iteration of American history, which never loses track of its realism-oriented framework, and always remains relevant to the American society it aims to reflect.
 
Watchmen depicts an alternate reality where the split between its universe and ours occurs in 1938. The emergence of the very first “superhero”, Hooded Justice, leads to the concept of masked vigilantes growing significantly popular and widespread. As the amount of superheroes starts growing in number and in influence, several different events in United States history are drastically altered in the process. For instance, a patriot-themed, far-right superhero dubbed the Comedian notoriously prevents the Watergate scandal from reaching the public, thus allowing Richard Nixon’s presidency to continue for five terms. Dr. Manhattan, a superhero who possesses a litany of god-like superhuman capabilities, is appropriated and used by the United States government in the Vietnam War, leading to the US's disturbingly quick victory and raising tensions with the Soviet Union. Already, we see that Watchmen establishes a theoretical moral framework for its superheroes. Many of the actions they take range from ethically ambiguous to outright criminal, evoking a ""persistent uncertainty as to whether [these] superheroes should pursue their self-imposed obligation for the pursuit of [a]... subjectively defined 'justice'"" (Giddens).
 
In 1977, after a significant amount of uproar from law enforcement officials emerges, claiming that the existence of superheroes now threatens their careers, the Keene Act is signed, banning all heroes but the Comedian and Dr. Manhattan from ever working publicly again; the final nail in the coffin for a decades-long legacy of widely mocked vigilantism. The graphic novel opens in 1986, where the Comedian has been killed and viciously thrown out the window of his apartment by an unseen assailant. In the aftermath, Rorschach, a rogue, terribly unstable, and similarly far-right vigilante who refused to abide by the Keene Act, decides to investigate his murder. With the help of several different ex-heroes — the tech-centric Nite Owl, former sex symbol Silk Spectre, ex-superhero narcissist billionaire Ozymandias, and Dr. Manhattan himself — the group ultimately ends up uncovering a Cold War-embroiled conspiracy that threatens not only the remaining ex-superheroes, but also millions of innocent civilian lives.
 
Having covered the nuances of Watchmen’s alternate reality and its relatively grounded approach to the idea of superheroes in general, it becomes worth noting that there are an exhaustive plethora of things worth covering about the graphic novel's impeccable artistry. For one, it captivatingly uses cinematic devices, motifs, and visuals that only truly work within the graphic novel medium, using numerous references to art and music that give an extra layer of meaning to already excellent scenes. Likewise, the unparalleled development and depth given to each of these heroes’ pasts, beliefs, and ever-constant fallibility is also a key benefit, as well as the inclusion of supplementary excerpts, articles, and news reports at the end of each chapter that offer further insight into Watchmen’s Cold War America. Most importantly, however, as Boston University's Jeffrey Wu claims, “The true sense of morality in Watchmen lies in its lack of a ‘true hero,’ of a ‘right and wrong,’ of a ‘correct’ answer. The world the Watchmen inhabit is dark, gloomy, and above all, vicious… In a single word, Watchmen is gothic"" (2). At its very core, Watchmen is a comic book about comic books, and its incredible mastery over the graphic novel medium and all of its foundational intricacies is an achievement not many modern works of art have exhibited or ever will exhibit.
 
However, despite this encyclopedia’s worth of information about Watchmen’s universe, story structure, and visual detail, a disturbingly low number of adaptations and continuations to Watchmen have managed to successfully grasp any of it, to a degree where even writer Alan Moore has publicly made known his own disillusionment with adapting his own work (Epstein). For instance, Zack Snyder’s 2009 film adaptation of Watchmen not only fails to understand what narrative devices the graphic novel utilizes to portray realistic modern superheroes as a rather pathetic concept, but also alters a significant portion of the graphic novel’s ending to a point where its original thematic resonance fails to make any coherent sense. Doomsday Clock, another serialized comic book sequel to Watchmen that ran from 2017 to 2019, places Watchmen's characters alongside the DC superheroes they deconstructed, construing an optimistic meaning out of Watchmen’s bleak story with a dysfunctional foundation.
 
Perhaps the strongest appeal to Watchmen that its adaptations and sequels have failed to grasp, however, is its incredible relevance to the Cold War era it was released in — its usage and alteration of actual historical events from the 1940s to the 1980s in order to reflect Cold War fears over nuclear holocaust was one of the main factors that made it such an excellent piece of work for its time. Perhaps its most important motif is the clock nearing midnight — an image popularized by the Bulletin of Atomic Scientists — as a visual device conveying the world's slow march towards utter catastrophe, always seen on the cusp of some disaster. In some ways, these themes still resonate today, as continued tensions over the existence of nuclear weapons ensure that these sorts of Cold War fears, although somewhat diluted, remain a constant afterthought in our collective subconscious. As such, Watchmen is a deliberate product of its era and has universally resonated with an audience of stalwart loyalty because of it — the core tenet that adaptational works such as Snyder’s Watchmen and Doomsday Clock have never really found a relevant place for. It is, in essence, a ""historiographic metafiction, [one that] challenges institutions in the readers' ontology and warns... about the dangers of taking a single perception of the world as the only possible reality"" (Ganteau 186).
 
With this general foundation in mind of what exactly makes Watchmen so appealing and seemingly impenetrable for adaptation, it now becomes worth addressing the context behind Damon Lindelof’s Watchmen HBO miniseries, a Watchmen continuation which premiered on October 20, 2019 to critical acclaim and numerous accolades. Its most impressive achievement and main point of acclaim was finally being the Watchmen sequel to fully comprehend what made the graphic novel tick.
 
~~~~~~~~~
 
On May 25, 2020, a Black man named George Floyd was murdered by the Minneapolis police in what would become not only the latest in a series of racially charged police killings, but also the impetus for the most energetic resurgence of the Black Lives Matter movement in recent memory. These drastic cultural developments have led to a somewhat widespread cultural reevaluation in America regarding the media we consume, how aware we as a country really are of systemic injustices, and to what extent media reflecting a racially charged American history can educate and inform us on its fundamental outcomes.
 
Nowhere is this reevaluation perhaps more poignant, however, than the very opening scene of HBO’s Watchmen being frequently mentioned for bringing awareness to a long-suppressed, real-life incident in America’s racial history. The incident in question is the Tulsa Race Massacre of 1921, in which several groups of deputized white residents of Tulsa, with the support of the Ku Klux Klan, viciously slaughtered dozens to hundreds of Black residents and burned down an entire Black-owned district (Lindelof and Kassell).
 
This is the backdrop for The Leftovers and Lost creator Damon Lindelof’s visionary modern interpretation of Watchmen — a story set to the idea of recontextualizing America’s history with racism through a modern remix and continuation of a timeless superhero story. Much like how the Cold War was an inextricable part of the graphic novel’s thematic argument, racism and its storied history are nigh-irreplaceable in the miniseries’ thematic thesis, and it strives for incredible lengths to offer what could well be the leading example of depicting racism in a mainstream superhero story.
 
2019’s Watchmen is, through and through, a story about legacy, trauma, and history, and the graphic novel is no exception to being a primary contributor to the long-lasting ramifications of this legacy. Thirty-one years after the incredible devastation left behind by the graphic novel’s bleak ending and ninety-five years after the events of the Tulsa Race Massacre, a white supremacist militant group known as the Seventh Kavalry emerges, inspired by Rorschach’s disjointed record of his 1986 investigation and revolted by reparations for victims of racial injustice in a left-leaning presidential administration. After months of preparation, the Kavalry stages a vicious attack on all police officers of color in the Tulsa Police Department, killing dozens of officers and causing almost all of the remaining survivors of the attack to depart from the force in an event known as the White Night.
 
As part of the three-year rebuilding process, however, new legislation mandates current police officers to don masks and shield their identities, which also permits the revitalized existence of “superheroes,” this time as costumed officers sanctioned under the law. The ensuing investigation and crackdown on the Seventh Kavalry — spearheaded by Angela “Sister Night” Abar, Wade “Looking Glass” Tillman, and police chief Judd Crawford— eventually leads down a spiraling rabbit hole of harsh betrayal, shocking revelations, and unspeakable bigotry. Old characters from the graphic novel are reintroduced, the origins of Watchmen’s superheroes in 1938 are truly given flesh, and American history is given a more significant role in the Watchmen canon than they have ever taken on before.
 
One example of the appeals of 2019’s Watchmen is its incredible understanding of the source material when it comes down to how it reincorporates the original characters into this new story. A more notable character who almost perfectly fits this bill would be Laurie “Silk Spectre II” Juspeczyk, the daughter of the first-generation Silk Spectre, Sally Juspeczyk, a vigilante and sex symbol who had once been sexually assaulted by the Comedian early on in her career. As the story continues on, Laurie’s somewhat optimistic worldview is gradually eroded by her true origins and the devastation left behind by the graphic novel’s ending, even as she learns the importance of nostalgia and love in her rather turbulent life. As such, in 2019, a much older Laurie — fantastically portrayed by a deeply layered performance from Jean Smart — takes on a much more sardonic and cynical persona as she actively adopts the role of hunting down non-sanctioned “superheroes” for a sect of the FBI.
 
However, her cynicism serves as a masquerade for her own pain, which stems from a deep sense of abandonment from the two men she loved most; Nite Owl and Dr. Manhattan. The former has been sent to prison, and the latter has departed the planet entirely, which crushes Laurie’s sense of self and ultimately causes her to act out of bitterness. Early on in the series, she joins the Tulsa Police Department’s task force against the Seventh Kavalry, and as the investigation progresses, she rediscovers important parts of her past, coming to the realization that her nostalgia for her relationships still remains. While her love for the two men in her life may be a satirization of oversexualized, paper-thinly characterized superheroines in the superhero genre, it eventually becomes a major part of how she heals from her trauma, putting on display the showrunners’ adept skill at managing to re-idealize grim deconstructions.
 
Many of the newer characters introduced in the miniseries also manage to follow the graphic novel’s psychologically insightful framework for character development. The series’ primary protagonist, Angela Abar — portrayed excellently by Regina King — serves as a very well-established example of this reflective narrative philosophy in action. One of the leading detectives in the Tulsa Police Department, Angela dons the persona of “Sister Night”, a character inspired by the lead in a fictional blaxploitation film from her traumatic childhood. After being led to a harrowing discovery regarding the Tulsa PD’s leadership by Will Reeves — an enigmatic Black centenarian survivor of the Tulsa Massacre with a long and a shockingly influential past — Angela is forced to come to terms with her own trauma and legacy throughout the investigation. Such a journey leads her all the way back to the origin of superheroes in an America undergoing racial turmoil in 1938, tying the war with racism being fought in the present back to a key part of its historical roots, and powerfully connecting both these systemic conflicts to the very origin of Watchmen’s masked vigilantes. Other characters fighting the Seventh Kavalry such as Looking Glass and Will Reeves receive similarly investigative and psychologically insightful character profiles, often having entire episodes devoted to them in the same fashion as the graphic novel’s chapters. Such attention to structural detail ensures that the miniseries’ framework for creating characters directly parallels and expands on that of its predecessor.
 
Adding onto all this, the Watchmen miniseries takes a distinctly significant number of cues from the graphic novel and almost perfectly reflects the minutiae of its impeccable artistry. It takes the graphic novel’s incredibly captivating usage of cinematic devices, motifs, and visuals and translates them straight to the medium of television, using references to the graphic novel, stage musicals such as Oklahoma!, and other forms of art that give an extra layer of meaning to already excellent scenes. It cultivates an incredible sense of development and depth given to both the new and existing heroes’ pasts, traumas, and fallibilities, while always maintaining a sense of deconstruction and homage. It includes supplementary materials in a website called Peteypedia, all of which give more insight into Watchmen’s racially divided America. Most importantly, however, it yet again deals heavily with moral ambivalence. Heroes are not what they seem, repressing their anger and shame behind masks, while villains are concealed behind the veneer of heroism and nobility.
 
As such, Lindelof and the miniseries’ creators’ understanding of the graphic novel’s minutiae reveals not only their devotion to comprehending its artistry, but also their undeniably keen desire to create the superhero genre's own historiographic metafiction of the 2010s. We find ourselves in a modern era where dozens of Black men and women shot and killed by police cannot find justice; an era where fanatical bigotry is seeing a revival in existing neo-fascist groups reflected through the Seventh Kavalry; an era where systemic issues such as redlining, racial housing quotas, and circumvention of anti-discriminatory laws lead to disproportionate poverty for people of color; an era where the anger and suffering caused by racism have forced people onto the streets even in the midst of a global pandemic, and the list continues on. The Watchmen miniseries, much like its predecessor, reveals a striking awareness of this history, and even if it alters many of these events through the lens of science-fiction, it is not without purpose and never seems completely outlandish in doing so.
 
Through recontextualizing racism through science fiction while retaining the necessary historical context, the Watchmen miniseries truly reaffirms the Atlas-esque social and political weight of its own title. It's an ornately detailed and deconstructive superhero story with an inextricable connection to the systemic concerns of the era it exists in, whether that be the existential dread of the Cold War and its numerous administrative failures, or the eternal conflict with racism and its vile permutations. Nothing ever ends, and Watchmen will forever be one of the greatest artistic representations of that constant eternity, no matter what we choose to make of it."
"What are the fundamental axioms of Blackness? How do they dictate the Black experience? Toni Morrison investigates these questions in her novel Jazz, where she follows the journeys of various characters who grapple with the nature of their personal and racial identities. In doing so, Morrison explores the fundamental nature of whiteness and Blackness, specifically through Golden Grey, arguing that the structures of whiteness and Blackness live in a coexistent, codependent, but unequal dichotomy—such unequal dichotomy is what allows white oppression to thrive. White racial subjugation is  a continuous theme strung throughout the text, eventually leading to metaphorical depictions of the bustling Black communities of Harlem, through which Morrison argues that Black existence must continually evolve in the face of oppressive white institutionalism. These ideas manifest themselves in the sensation of fluidity, or the feeling of a divergent identity caused by the oppressive structure of the dominant white race, which is intrinsically unique to the African American experience as the dehumanization of Blackness in society is what allows whiteness as a dominant power to exist. Although this need to be fluid supposedly eradicates the ability for one to possess a unified sense of self, Morrison offers that unification comes instead through acceptance. Ultimately, Morrison argues that stability is rooted in the acceptance of a fundamentally unstable, ever-changing form of Blackness—thus, hope and power can be found within contradiction.
Through the journey of Golden Grey, Morrison argues that the structures of whiteness and Blackness are dependent on each other—the existence of Blackness allows for whiteness to exist not simply as a social construct but also as an ideal or dream for Black people—in this way, embodying Blackness was to embody a burden. Golden manifests the personification of societal whiteness because his identity as a white person is fabricated: although he is racially both Black and white, he identifies as and is only accepted through his whiteness. When Golden was born, True Belle and Vera Louise were ecstatic by his white appearance: “True Belle told him Vera Louise had smiled and said, ‘But he’s golden. Completely golden!’ So they named him that and didn’t take him to the Catholic Foundling Hospital, where white girls deposited their mortification” (148). True Belle’s reaction upon seeing Golden’s hair for the first time was a smile, indicating that Golden’s “complete goldenness” was a source of relief for both True Belle and Vera Louise. This suggests that if Golden had appeared Black, it would be a source of restlessness. Morrison uses the word “but” in the beginning of Vera Louise’s dialogue to signify the dismissal of Golden’s Blackness which can only be done because Golden appears to be white; this idea is then further highlighted by True Belle and Vera Louise’s decision to not go to the Catholic Foundling Hospital, a place where white girls are shamed for their mixed-heritage babies. Their refusal to go to the hospital shows that Vera Louise and True Belle didn't feel the need to be ashamed about Golden because Golden embodies whiteness. Their decision to name Golden after his golden hair demonstrates that his white characteristics are what define him. Morrison utilizes the idea that Golden was a “white baby” to argue that cultural whiteness and being genetically white are systematically different from each other—one doesn’t need to be completely white to embody whiteness, and vice versa. This characterization is further elaborated upon during Golden’s interaction with his father, who says: “‘Look. Be what you want—white or Black. Choose. But if you choose Black, you got to act Black, meaning draw your manhood up—quicklike, and don’t bring me no whiteboy sass” (173). Golden’s father preemptively stamps the “white” label on Golden by indicating that Golden could “choose to be Black” if he wanted to, moreover, if he tried. The father’s emphasis on Golden having to “act Black” and not to bring any “whiteboy sass” if he wanted to be Black shows that Golden must make a conscious effort to embody Blackness but not to embody whiteness because he is inherently defined by his white appearance. Golden’s father characterizes whiteness and Blackness as polarized structures unable to reconcile—because Golden’s father believed that Blackness is characterized by certain actions or thoughts, to embody whiteness was to lack those characteristics. In this way, Morrison argues that whiteness and Blackness are relative constructs: because white identity is dependent on not embodying Blackness and Blackness is dependent on not embodying whiteness, the existence of these structures rely on each other. This relationship, although contingent on dependency, is also unequal: the vitality of whiteness is founded on its ability to characterize the embodiment of Blackness as a burden. The fact that Golden is perceived as a white character throughout the novel is a tangible example of this: Golden’s embodiment of whiteness doesn’t come from his identity as a fully white person—he’s only half white—instead, his whiteness is founded on the fundamental belief that he’s not Black. As shown through his birth and his interaction with his father, Golden’s whiteness is contingent upon other people’s view of him as white— his white identity is not rooted in his own conscious decision, but rather a label that was placed on him by others beginning at birth, a part of his life when he had no control. Golden’s whiteness is not natural, it is a product of his surroundings: thus, whiteness is not only a social construct, but the ultimate manifestation of Black subjugation. 
Golden’s two-dimensional perceptions of whiteness and Blackness are what contribute to his lack of awareness, revealing that he uses societal constructs of Blackness to subjugate and confine Black people as a means to push his own simplistic narrative. Golden is conditioned to view society through a stark Black and white lens because that was all he was exposed to from birth. He subjugates other characters through his own understanding of what it means to be Black or white. Societal ideas of whiteness and Blackness invade his perception, blinding him from the true complexities of race and status. When Golden encounters Wild, a Black woman who is hurt after falling from a tree, he reacts in dismissal: “He wants nothing to do with what he has seen—in fact he is certain that what he is running from is not a real woman but a vision” (144). He describes Wild’s eyes as “large” and “terrible,” showing how he equates Blackness with disgust. Golden forces himself to be “certain that what he was running from is not a real woman but a vision,” highlighting how he wants his perceptions of Blackness to remain extraordinarily concrete. However, Golden faces a dilemma when he recognizes that his horse is Black too: “he can't help noticing that his horse is also black, naked and shiny wet, and his feelings about the horse are of security and affection. It occurs to him that there is something odd about that: the pride he takes in his horse, the nausea the woman provoked” (144). Golden’s positive feelings towards the horse stemmed from the fact that his horse drives him forward. He describes the horse as “black,” “naked,” and “shiny wet,” taking on an almost sexualized tone—the horse becomes like a deity, moreover, a being of obsessive admiration. Through these descriptions, orrison juxtaposes Golden’s “pride” for his horse and his “nausea” towards Wild. Golden finding both “security” and “disgust” in Blackness simultaneously contradicts his one-dimensional perception of what Blackness is. In order to relieve this contradiction, Golden forces himself to see Wild as merely a vision, allowing him to dismiss her humanity and also dismiss any belief that Blackness could be complex. His desire to do this is rooted inwards, by Golden’s perceptions of his own identity: Golden simplifies his ideas of Blackness because his simplification of Blackness is what gives him power. 
Whiteness and Blackness exist in a codependent, but unequal dichotomy; this means that whiteness thrives on its subjugation of Blackness, while Blackness remains confined in a white-imposed one-dimensionalism. This subjugation of Blackness creates a societally defined need for fluidity because it is the only way Blackness can exist beyond white-imposed barriers. While fluidity should give Blackness power, it also creates a lack of personal self identity amongst Black people. This idea is most present in Morrison’s characterization of Harlem, a predominantly Black city: ""Really, there is no contradiction— rather it’s a condition: the range of what an artful city can do… tar softens under the heel and darkness under bridges changes from gloom to cooling shade. After a light rain, when the slaves have come, tree limbs are like wet fingers playing in woolly green hair. Motorcars become Black jet boxes gliding behind hoodilights weakened by mist (117)."" The fluidity of the City represents the transformation of Black character: just as the objects in the city transform in a way that departs from its original nature, Blackness transforms, taking on new facets when necessary. Morrison emphasizes that the transformation of the city is not a “contradiction” but rather a “condition,” showing that different forms of Blackness exist in tandem with each other because the ability to transform is fundamental, not destructive. 
Morrison argues that as time passes, conditions change as well—Blackness can evolve within a person. The constant evolution of Blackness, although necessary, also drifts Black people from their personal sense of self. This is demonstrated in Violet whose fragmented identity is exemplified by the distinctions she makes between different parts of her persona: Violet herself and a violent, impulsive Violet (that Violet). When recalling the day she tried to attack the corpse of a murdered Dorcas, Violet distances herself from those actions by telling her story through the eyes of that Violet: “By then the usher boys were joined by drowning men who carried that kicking, growling Violet out while she looked on in amazement” (92). Morrison uses the word “she” in reference to Violet’s remembrance of this scene to signify that Violet and that Violet exist as different characters not only in time, but in physical space..  This narrative continues as Violet conveys the rest of her story through the same lens: “But if that Violet was strong and had hips, why was she proud of trying to kill a dead girl, and she was proud. Whenever she thought about that Violet, and what that Violet saw through her own eyes, she knew there was no shame in there, no disgust” (94). Morrison uses these distinctions to show that Violet and that Violet are fundamentally different characters. She deliberately writes her past narrative in third person, highlighting the distance she feels from that Violet. The phrase “her own eyes” also indicates that although Violet and that Violet technically exist in the same body, they are physically distinguishable. Consequently, Violet is unable to reconcile parts of her identity that she must claim as her own, because her unified sense of self is lost through her need to be fluid. 
When Violet is able to reconcile fragment pieces of her identity and unify her sense of self, she gains power. When Violet is able to reconcile her identity as both Violet and that Violet while reflecting on her past memories, she is fundamentally changed: “No! That Violet was not somebody walking round town, and down the streets wearing my skin and using my eyes s*** no that Violet was me! The me that hauled hay in Virginia and handled a four-mule team in the brace” (95-6). Violet realizes that not only are Violet and Violent both one in the same, but that both identities were equally impactful in shaping her identity. Violet’s mention of her life in Virginia shows how her co-existence as both Violet and that Violet existed far longer than her altercation at Dorcas’s funeral, but rather was in intrinsic part of her character. Violet doesn’t reject her multifaceted identity but rather embodies it, illustrating the power and significance of acceptance. 
Violet’s ultimate reconciliation ends on a hopeful note of laughter, which Morrison uses to show the lingering, yet strong hope that exists in those who find reconciliation within themselves. Upon coming to her realization, Violet has a conversation with True Belle, Rose Dear, and Alice, which soon gets derailed by a scene of laughter: “True Belle looked at them and had to lean against the wall to keep her laughter from pulling her down to the floor with them. They should have hated her... But what they felt was better. Not beaten, not lost. Better. They laughed too, even Rose Dear shook her head and smiled, and suddenly the world was right side up. Violet learned then what she had forgotten until this moment: That laughter is serious. More complicated, more serious than tears.” (113) Morrison emphasizes the power of laughter through her emphasis on True Belle: the phrase “they should have hated her,” exemplifies the fact that despite the tension between True Belle and her sisters, the power of laughter overcame any feelings of hate. Morrison indicates that the characters didn't feel “beaten,” showing that optimism is rooted in resilience. When Rose Dear begins to laugh alongside them, the world goes “right side up,” indicating that optimism brings stability and order. Thus, stability is found through the acceptance of fluidity because it is intrinsic to Blackness. 
    Toni Morrison’s Jazz is an exploration of Black characters that are presented with no other choice but to be fluid. Identities are rooted in sociological perception. Race and the self are societally constructed—Morrison argues that Blackness and whiteness exist not as natural nor metaphysical concepts, void for the influence of time and space, morphing with culture and their existence with each other. This is what makes gaining systemic power so hard for Black people, as they were never meant to have tangible power in a society that relies on Black subjugation to fuel the existence of whiteness. Thus, Morrison points out that they must find their power elsewhere, through the acceptance of the identity they were subjugated to. Through acceptance, there is power. Violet only comes to this realization when she accepts that inherent contradiction: she finds stability in accepting her fluidity, a sentiment that is intrinsically unstable. Ultimately, Morrison leaves the reader with a glimmer of hope: although the structures of race remain unchanging, and white oppression will not die, the reclamation of power through Black identity is what allows even the most emotionally wrecked characters to be optimistic in a racially-torn world. "
" Whenever I travel to a foreign country, my American passport reveals my identity. Gold eagle stamped on navy blue. Carrying it to visit relatives in China, I spoke English with my sister to distinguish myself as American. I, with my childish pride, knew exactly who I was: American. And therefore, despite our similar physical features, I was pointedly different from the other children I caught frogs and tadpoles with over the summer.
    I love the familiarity of touching down on the airport runway. I love breezing past the endless line for foreign arrivals. Most of all, I love the crisp stamp and a “Welcome home!” from the U.S. customs agent. 
    However, not all other citizens, or even the president of our country, are as welcoming to my return. President Donald Trump’s designation of COVID-19 as the “Chinese Virus” makes this particularly difficult. Though the logic behind calling the virus “Chinese” would link it to the initial cases reported in Wuhan, as China was the first to weather the pandemic, this rationale fails to account for how the term “Chinese Virus” actually translates to society, where fear and confusion offer up Asian-Americans as an easy target for hate crimes. Even with the large Asian-American population in the San Francisco Bay Area, fear and unjustified hatred can be found no further than my local community, where five Asian-owned businesses were left with shattered glass doors and windows. According to San Francisco Chronicle, roughly 600 incidents of racism and discrimination against Asian-Americans and Pacific Islanders related to COVID-19 took place in April in California–close to half of those were in the Bay Area (Sanchez). A quick search pulls up videos of those of East Asian descent chased with spray bottles in New York, berated on public transportation for wearing a face mask in Maryland, criminalized for coughing or waking up to a vehicle vandalized with racial slurs in Boston. 
    Calling COVID-19 the “Chinese Virus” blames the pandemic on a whole culture, their descendants and others who resemble their physical appearance. The generalization of the disease is a careless stigmatization of all Asian-Americans. Not every Chinese-American has COVID-19, and not everyone who has tested positive is of Chinese descent. In fact, the flawed logic of the term “Chinese Virus” is just one ugly stereotyping example that has been imposed on the ethnic and cultural heritage of Asian-Americans to detract them from acceptance as American.
    To confront the coronavirus and alienation, the Chinese-American community has gone to great lengths to mobilize in slowing the virus’s spread, supporting local hospitals, donating masks and 3D printing face shields for doctors and nurses. According to the San Jose Spotlight in the San Francisco Bay Area, more than 25 Chinese-American groups had collectively fundraised over $930,000 to purchase medical supplies in the month of May by using their overseas connections to help fill a supply shortage felt throughout the country (Bitters).
    However, there is a distinct difference between donating out of will and donating as a plea to be accepted as American. My community’s good will is misunderstood by some as a submissive gesture from the “model minority” to please a rigid system of stereotypes. Indeed, former Democratic presidential candidate Andrew Yang advocated Asians to do just that–fulfill any stereotype demanded necessary for acceptance. In an opinion article published in the Washington Post on April 1, he encourages Asian-Americans to “show our American-ness in ways we never have before,” and suggests that the community “wear red white and blue” in order to “show without a shadow of a doubt that we are Americans who will do our part for our country” (Yang). He insists that due to their ethnic heritage, Asian-Americans must vigorously volunteer, enthusiastically wave flags, spin their tale into one of diehard patriotism, just to grasp at what their white neighbors naturally possess, the title of “American.” Yang incorrectly assumes that acceptance is something to be earned by assimilation according to the dominant order, furthering the false narrative that Asian-Americans are inherently foreign to American society.
    As novelist Toni Morrison pointed out the truth of this strained effort to prove one’s Americanness, her words, “in this country American means white. Everybody else has to hyphenate,” still ring true. By encouraging Asian-Americans to do more to show their patriotism, Yang reinforces the idea that hyphenation does not mean fully American or fully patriotic, but instead connotes a degree of separation from what it means to be American. This gap that should not exist nevertheless does, as seen over the course of history.
    Asian-Americans have long been pinned under the title of ""perpetual foreigner"" due to their “oriental” features and cultural heritage starting from the arrival of the earliest laborers in California. Codified as Yellow Peril, the fear of the Oriental, non-white other, dates back to the 1850s. Legislation strengthened the social consensus that peoples of Asian ancestry were incapable of assimilation and even less so acceptance. Laws such as the 1882 Chinese Exclusion Act, which prohibited the immigration of Chinese laborers, and other stringent restrictions on Asian immigrants in the 1910s and 1920s such as the Asiatic Barred Zone Act of 1917, regulated even the existence of the Asian minority. As such, public perception of Asian-Americans was restricted to the monolithic Yellow Peril.
    During World War II, after the unexpected attack on Pearl Harbor on Dec. 7, 1941, Asian-Americans desperately needed to distinguish between the two parts of their hyphenation (Takaki). Their heritage was Asian, but they were American. White fear of internal enemies strengthened, and the country’s mindset was geared toward — often irrationally — pointing out “aliens” among the population.
    This fear manifested in American public and government hurling undeserving retributions at Japanese-Americans, accusing them of supporting the Japanese government. In the name of “military necessity,” 120,000 Japanese-Americans, from adults to infants and mostly from the Pacific mainland, were removed from familiar sidewalks and loved hometowns to barren barracks furnished with Army cots and barbed wire fences (Takaki).
    The easing of government policies in the 1940s and 1950s did not result in an embracing inclusion of Asian-Americans in the nation. As the country determined how to incorporate Asian-Americans as new citizens, a new stereotype was invented, and Asian-Americans underwent a metamorphosis by the mid-1960s as the well assimilated and politically non-threatening model minority. This dominant rhetoric cited color line and racial (often coded as cultural) difference of Japanese and Chinese to explain postwar Asian-American socioeconomic mobility, re-marking them as non-white, foreign others, effectively creating new modes of exclusion and justifying social inequalities. (Wu)
    In such an environment, it is easy to forget that Asian-American have toiled tirelessly for America. Chinese immigrants endured back-breaking labor and uncounted casualties building the Transcontinental Railroad from 1863 to 1869. The 442nd Infantry Regiment, the most decorated military unit in American history composed of mostly second generation Japanese American soldiers, suffered 9,486 casualties during World War II. Even after countless Asian-Americans have labored and fought for this country, societal imperatives and narrative easily casts them aside as strange others in times of crisis. 
    In the late 20th century, Asian-American entrepreneurs have pioneered products for the 21st century. Yahoo, a company co-founded by Taiwanese-American programmer Jerry Yang, led the way during the onset of the Internet. NVIDIA, a company co-founded by Taiwanese-American Jensen Huang, produces processing units powering gaming software such as Minecraft. Zoom founder Eric Yuan heads a video conferencing platform used by schools, businesses and friendly gatherings throughout the pandemic. Asian-American leadership has paved the road of innovation and should not be forgotten in favor of misplaced fear. Not only have Asian-American immigrants chosen this country for its wealth of opportunities, but they have also in turn built new foundations. 
    It is not the responsibility of Asian-Americans to prove their “American-ness”. As Ronald Takaki, a former professor of Ethnic Studies at UC Berkeley, writes in “Strangers from a Different Shore,” acknowledging this country’s multicultural immigrants and diversity “would require a critical scrutiny of American society”. As citizens of this country, we, born here or naturalized, are obligated to join the collective effort to stop the virus. It is also our responsibility to call out another form of pathogens in our systems and structures. The use of “Chinese Virus”, rooted in an ugly history of racial and cultural derision, not only undermines our civility and causes social divisions but also comes at a cost to human lives during a pandemic.
    If I stood out due to my American nationality during childhood summers in China, and if neighbors of the same nationality now solely regard me by my ethnic heritage, looking past my American identity without a thought, where do I belong? To me, this is home. You and I are both Americans, featured differently, with diverse cross-cultural roots, but committed equally to the well-being of our country. I should be able to use chopsticks and speak in Mandarin and have pride in my cultural legacy without compromising my identity as an American citizen. I display my heritage and nationality by watching the Spring Festival Gala broadcast on Chinese television during Lunar New Year, munching on a plate of dumplings, and driving up to San Francisco five months later to cheer with strangers, united under fireworks on the Fourth of July.
    In that crowd, picture an American. Picture a Chinese person. Now, picture a girl, a product of these two cultures, sitting on a roof with a smile stretching so wide her cheeks hurt as the night rumbles awake in bursts of red, white and blue. The dark hides her black hair, her black eyes, and her tinted skin. It hides how eyeshadow refuses to pack into the crease of her upper lid as YouTube videos demonstrate, but her silhouette against the sky frames the outline of a patriot.
    Now, picture Americans, you and me. "

"For the past five months, American society has been restrained from its normal level of social interaction, whether that be in the workplace, at school, or between families and friends who don’t live together. Simultaneously, the country, and the world, has been through momentous upheaval, with COVID-19 exposing government incompetence or authoritarianism, civilians gathering in the streets to protest or riot against perceived injustices, supply chains and small businesses all but collapsing, human rights crises and chemical explosions demanding action. 

Now more than ever, without routinely engaging activities that necessitate focus away from our phones and computers, we are drowned in constant notifications of people being shot in the street, dying in hospitals, being discriminated against, facing homelessness and bankruptcy. As social creatures who view empathy as a virtue, humans (especially the youth with our volatile temperaments) feel obligated to constantly feel in response to everything- in the past few months, it’s more often than not outrage, sadness, disgust, despair. This impulse and these emotions are amplified by social media, a platform too accessible to have excuses to not participate in through sharing graphics, photographs, pseudo-manifestos, regarding how to, essentially, showcase caring the most. 

This creates a saddening paradox- we’re expected to feel everything, since not feeling supposedly is a form of complacency or perpetuating injustice. Yet, we can’t do anything tangibly significant to mitigate inevitably media-hyperbolized deaths and suffering through racism, COVID19, economic ruin. So we are left to simply stew in the emotions of others and ourselves through a screen for days, weeks, now months on end, as the world continues on its apparent highway to hell. 

The prolonged digital nature of news consumption and activism leads to desensitization to the issues we claim to care the most about, excarberating the exchange of negativity in the context of our political and cultural discourse. 

Substantial research indicates that when people are exposed repeatedly to the same stimulus, their response decreases over time on a neurological basis, specifically in the magnitude of hypothalamic-pituitary-adrenal activation (HPA), or our psychological and physical response to stress. This process is named habituation. Habituation speeds up in proportion to the frequency of exposure, and causes us to generalize stress-inducing stimuli along with our diluted responses. As of 2019, Americans check their phones an average of 63 times a day, with teens, the age demographic most likely participate in online activism, averaging over seven hours on their phones, so frequency of exposure is definitely over-fulfilled. As for generalization of stimuli, this can clearly be seen in the rise of tribalism and polarization skyrocketing steadily in the wake of social media, and even more so in the past few months of national unrest. Disagreeing with someone in political matters almost always ends a conversation, straining or severing relationships. Language fractalizes into dichotomies between good and evil devoid of nuance (COVID19 justifies lengthy lockdowns and economic shutdown, or is unjustifiably exaggerated as a health threat); all cops are bastards, or they’re all pure-hearted heroes; demonstrations of all kinds are justified, or all protests are to be condemned for allowing violence; systemic racism is a fundamental factor in outcome or a victimhood myth, etc. Clearly, America today enables habituation to occur on a huge scale. 

Multiple studies support habituation as a response to exposure to media. A study where participants watched nine violent movie scenes and nine comedy scenes found that repeated exposure to violence reduces its psychological impact in the short term, and at a higher rate than desensitization to comedy. Another study, focusing specifically on violence in the news and entertainment, using surveys from three regions of the US, found that heavy local news consumption correlates with a “blunted response to news stories regarding real-life violent events when individuals have low trait empathy.” Research from the University of Michigan followed college students over 30 years and found a 40% decrease in empathetic capabilities during this span, with the sharpest decrease after 2000, when digital technology began to enter the mainstream American lifestyle. The researchers found that social media has caused us to understand one another less (we’re less able to communicate effectively through nonverbal cues, which comprises the majority of a message, and can’t understand other perspectives as well due to less exposure to them via echo chambers). The only exception to overall emotional dilution was anger, which increased in virality and magnitude through reactionism, tribalism, and the popularity of shame. In addition, research indicates that negative emotions are more contagious than positive ones because they’re rooted in survival instincts to avoid potential threats by “catching” someone else’s fear. 

Our current state - ceaseless exposure to the news and our peers, dozens of contentious issues to divide us, physical isolation demanding compensation via increased digital interaction - is the perfect recipe for the hyperactive activist culture we see today, where caring for every single victim of supposed police brutality, COVID19, economic breakdown, is mandatory, or else silence is “violence” or a “privilege.” Yet this expectation is psychologically impossible and alarmingly damaging to society’s functionality. 
The idea of compassion fatigue was first coined by Charles Figley, a professor of psychology and family therapy, who described it as “we have not been directly exposed to the trauma scene, but we hear the story told with such intensity, or we hear similar stories so often, or we have the gift and curse of extreme empathy and we suffer...Eventually, we lose a certain spark of optimism, humor and hope. We tire. We aren’t sick, but we aren’t ourselves.” This definition and concept was initially applied to caretakers and therapists in regards to becoming desensitized to trauma and tragedy through decades of hearing similarly weighty stories from their patients.

Compassion fatigue has been studied extensively in professional health and human services, and repeatedly result in reduced performance, absenteeism, and intentions to leave their jobs. In charities and nonprofits, volunteer turnover is extremely high because of the nature of the job- low pay for self sacrifice, in the name of empathy. Empathy has been empirically proven to be finite: the more you allocate to Thing X, the less you have for Thing Y, and feeling obligated to designate equally for a myriad of things results in feelings of emotional drainage and burden. Empathy’s limits also result in preferential empathy to those we perceive as similar to us in belief and goals, enabling the underpinnings of an aggressive us-them mentality. Most importantly, Harvard Business Review cites studies that prove that empathy easily results in eroded morals, where “people are more inclined to cheat when it serves another person,” and “it only gets worse when they empathize with another’s plight or feel the pain of someone who is treated unfairly: In those cases, they’re even more likely to lie, cheat, or steal to benefit that person.”11

Empathy, when pushed to its limits, is finite, preferential, and self-rationalizing, all of which are negative. We can see all three in America today. Quarantine initially resulted in increased levels of anxiety and restlessness, before surrendering to depression and emotional exhaustion, with an underlying sense of having no control. More recently, two or three weeks after the George Floyd protests, graphics and articles began to circulate at higher rates regarding how to combat activism burnout, spelling out symptoms like lethargy, anger and hopelessness, feelings of shame and guilt for not being able to inspire significant change. Although people were initially unified by denouncing the knee of Floyd’s neck for almost nine minutes, once looting and violence began to be mixed in with protests, society began to fracture and radicalize, two distinct camps emerging of people who believe America is fundamentally racist and despicable, and those who believe America is a country built on good principles that it has been working steadily towards fulfilling since the Revolution, with racism as a declining role as a factor in achievement. People from perceived “oppressed” classes are automatically more credible and respected, as well as people who claim to be their “allies” by following constantly shifting standards of thinking and behavior, under the name of “anti-racism.” 
Self-rationalization is the most dangerous dimension of empathy. This can be seen in the justification of violence against law enforcement and non-protestors, gatherings en masse via protests being acceptable but not for parties or religion, cancel culture’s spread, warping data about black on black crime and police brutality, unilaterally condemning all white people for more implicit bias than any other race, and the notion of 2+2 = 4 being racist. As irrational, extreme things like this become acceptable in the same of social justice and equality, their overarching sentiment effectively pushes any dissent onto the defensive and into a minority, especially as they purge themselves by constantly pushing the idea that whatever anyone is doing to combat racism is never enough. 
Of course, empathy is a laudable social and neurological capability. It allows humans to strengthen our bonds with those around us, form connections with those who disagree with us through establishing other commonalities, and generally interact with people in a more positive and cooperative way. However, this great part of human nature is also the most vulnerable to exhaustion and perversion. 
Deep concern for the future of this country is warranted if legitimately toxic and unrealistic expectations continue to dictate activism. Taking breaks from consuming media and posting responses to current events needs to be normalized as a biological need - instead of looked down upon as being lazy or apathetic - in order to prevent emotional deterioration, mistreatment of others, and illogical radicalization. Mantras like “silence is violence” and “in the age of information, ignorance is a choice” can’t be accepted anymore: they set people up for (1) failure to be adequate in their ability to recite the names of every perceived victim of racism, power-abusing police officer or politician, not-woke-enough public figure, and (2) compassion fatigue that takes a personal toll and damages a person’s relationships. 
Ideally, our reserves of empathy should be redirected towards bridging gaps rather than reinforcing in-group biases, so that more of our limited energy goes moving together towards a shared vision of progress, rather than retreating further into our respective ideological corners in a delusion of complete self righteousness. "
"For the successive months after the pandemic first presented itself in New York City, we ate our favorite restaurant entrees out of plastic takeout containers, with cloth-bundled silverware exchanged for single-use cutlery and a bottomless bag of napkins. The time when we would again be served in-person platters and aromatic spreads no longer seemed within close reach. As we patiently endured springtime in the city, counties in Connecticut and New Jersey gradually received an okay to re-open dining services, while New York City eateries watched in understanding but deep envy. Days under lockdown grew into weeks and months, and by June 22, Cuomo cleared New York City—the former global epicenter—for outdoor dining. The precautions that disallowed indoor service from resuming, to the dismay of struggling restaurants, were drawn from Cuomo’s fear about the warning signs “from other states on the horizon.”

In anticipation of a sudden clamor for outdoor dining tables, restaurateurs scrambled to erect outdoor seating and barriers for the street space adjacent to their restaurants, transforming the city sidewalks into bustling woodworking stations. While the brick and mortar eateries were back in business, the decision to grant restaurants free rein over street space was yet another blow to New York City’s forgotten food population: street vendors. The temporary outdoor dining program, introduced at the New York City Council, has permitted restaurants to occupy plazas, sidewalks, streets, and parking lots—in many cases displacing vendor carts and street-food trucks that had been serving out of these very places for years. Forced to relocate, vendors lost many of their day-to-day patrons, a slowdown that was further perpetuated by a drastic reduction in foot traffic throughout the city during the three-month lockdown. The network of vendor carts around my high school, Stuyvesant, for one, rely on a midday rush of students—a demographic of customers who have not spent a dime at these same carts since the March closure of the school building. 

Vendors have already been economically underserved by the government throughout the course of the pandemic: a large fraction of the city vendor population is comprised of undocumented immigrants who were not afforded stimulus checks back in April. A resilient group of 20,000 hardworking individuals who serve much of the working class in the city, vendors are rarely represented in local legislation and are often overlooked by lawmakers in Albany. This precedent of hostility toward street vendors was cemented by a 1981 law that capped the number of street vendor permits at 5,000, essentially whittling down the industry to an elite hobby, not a passage for the American Dream. The mismanaged vendor license system, which emerged out of this antiquated legislation, has forced aspiring vendors to pay up to $18,000 for a retail $200 permit in order to sell legally out of their carts. The strict cap on vendor permit availability, which has remained unchanged for four decades, has not kept pace with the extensive population growth of the city and has authorized over-policing of vendors. Evelia Coyotzi, a tamale vendor in Corona, Queens, was arrested more than 15 times, not for breaking well-defined laws and rules, but for selling her authentic Tlaxcalan cuisine in the wrong place at the wrong time—namely, an era during which the police and former mayor Rudy Giuliani reportedly did not want a single vendor on the streets. 

A community that has long been subject to Albany’s cold shoulder, street vendors now have a chance to pass legislation that would support the industry in a period of unprecedented need—Intro 1116. Council members Margaret Chin and Carlos Menchaca pioneered a bill that would “aim to bring increased opportunities, fairness, and consistent enforcement to a chaotic system created by a decades-old cap that has forced many vendors to turn to an underground market for licenses.” Intro 1116 would directly expand the availability of vending permits, implement an official vendor advisory board, and create an office for street vendor enforcement. 

As a New York City student with a fixed budget, I understand that the street vendor population in our city is indispensable; my go-to orders at the halal and Korean food carts have stayed fresh on my mind even after a six-month hiatus. The vendors themselves were more than just faces I would pass as I trekked down the white-collar neighborhood of TriBeCa—I exchanged daily greetings and smiles, and they knew my order at the mere sight of me. For all of the affordable lunches that they served to me, it is now my obligation to support the bill that might very well keep their businesses alive in these troubled times. These vendors deserve more than our thoughts and prayers—they need decisive action. 

If your life has been bettered by a street vendor, or if you want to protect these resolute workers with families and dreams, I urge you to take to the following actions: call your council member and urge them to support Intro 1116; call Corey Johnson, the speaker of the New York City Council, and remind him that you care about street vendors in our city; and, finally, try to buy your next meal from a local street vendor. They have always been there for us at our every convenience—now imagine a New York City without them."